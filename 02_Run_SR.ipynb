{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2022 Flatiron Machine Learning x Science Summer School\n",
    "\n",
    "## Step 2: Run symbolic regression on generic data\n",
    "\n",
    "In this step, we investigate whether symbolic regression can discover the functions underlying the generic data created in Step 1. There is no support from deep learning at this point.\n",
    "\n",
    "We use the symbolic regression package [PySR](https://github.com/MilesCranmer/PySR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "from pysr import PySRRegressor\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.1: Load data\n",
    "\n",
    "We load the input and target function data created in Step 1 and create masks to split the data into training, validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data\"\n",
    "data_ext = \".gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded F01 data.\n",
      "Loaded F02 data.\n",
      "Loaded F03 data.\n",
      "Loaded F04 data.\n",
      "Loaded F05 data.\n",
      "Loaded G01 data.\n",
      "Loaded G02 data.\n",
      "Loaded G03 data.\n",
      "Loaded G04 data.\n",
      "Loaded G05 data.\n",
      "Loaded X01 data.\n",
      "Loaded X02 data.\n",
      "Loaded X03 data.\n",
      "Loaded X04 data.\n",
      "Loaded X05 data.\n"
     ]
    }
   ],
   "source": [
    "data = {}\n",
    "for file_name in os.listdir(data_path):\n",
    "    if file_name[-len(data_ext):] == data_ext:\n",
    "        \n",
    "        var = file_name[:-len(data_ext)]\n",
    "        var_data = np.loadtxt(os.path.join(data_path, file_name))\n",
    "        if len(var_data.shape) == 1:\n",
    "            var_data = var_data.reshape(-1,1)\n",
    "        data[var] = var_data\n",
    "\n",
    "        print(f\"Loaded {var} data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "\n",
    "train_size = 0.7\n",
    "val_size = 0.2\n",
    "\n",
    "mask_ext = \".mask\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created masks for X01 data.\n",
      "Created masks for X02 data.\n",
      "Created masks for X03 data.\n",
      "Created masks for X04 data.\n",
      "Created masks for X05 data.\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "\n",
    "masks = {}\n",
    "for var in data:\n",
    "    if var[0] == \"X\":\n",
    "\n",
    "        mask_name = var + mask_ext\n",
    "        try:\n",
    "            masks[var] = joblib.load(os.path.join(data_path, mask_name))\n",
    "            print(f\"Loaded masks for {var} data.\")\n",
    "        except:\n",
    "            data_size = data[var].shape[0]\n",
    "\n",
    "            data_idx = np.arange(data_size)\n",
    "            np.random.shuffle(data_idx)\n",
    "\n",
    "            train_idx = int(data_size*train_size)\n",
    "            val_idx = train_idx + int(data_size*val_size)\n",
    "\n",
    "            masks[var] = {\n",
    "                \"train\": data_idx[:train_idx],\n",
    "                \"val\": data_idx[train_idx:val_idx],\n",
    "                \"test\": data_idx[val_idx:],\n",
    "            }\n",
    "    \n",
    "            joblib.dump(masks[var], os.path.join(data_path, mask_name))\n",
    "\n",
    "            print(f\"Created masks for {var} data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.2: Run PySR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(X, y):\n",
    "\n",
    "    model = PySRRegressor(\n",
    "        procs=4,\n",
    "        populations=30,\n",
    "        niterations=30,\n",
    "        maxsize=20,\n",
    "        binary_operators=[\"plus\", \"sub\", \"mult\"],\n",
    "        unary_operators=[\"sin\", \"cos\", \"exp\", \"log_abs\"],      \n",
    "        model_selection=\"best\",\n",
    "        verbosity=0\n",
    "    )\n",
    "\n",
    "    model.fit(X, y)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"models\"\n",
    "model_name = \"pysr_models_v1.pkl\"\n",
    "\n",
    "try:\n",
    "    models = joblib.load(os.path.join(model_path, model_name))\n",
    "except:\n",
    "    models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning F01_0(G01).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: ignoring conflicting import of CoreModule.div into SymbolicRegression\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning F01_0(X01).\n",
      "Learning G01_0(X01).\n",
      "Learning F02_0(G02).\n",
      "Learning F02_0(X02).\n",
      "Learning G02_0(X02).\n",
      "Learning G02_1(X02).\n",
      "Learning G02_2(X02).\n",
      "Learning F03_0(G03).\n",
      "Learning F03_0(X03).\n",
      "Learning G03_0(X03).\n",
      "Learning F04_0(G04).\n",
      "Learning F04_0(X04).\n",
      "Learning G04_0(X04).\n",
      "Learning G04_1(X04).\n",
      "Learning G04_2(X04).\n",
      "Learning F05_0(G05).\n",
      "Learning F05_0(X05).\n",
      "Learning G05_0(X05).\n",
      "Learning G05_1(X05).\n",
      "Learning G05_2(X05).\n"
     ]
    }
   ],
   "source": [
    "for var in sorted([k for k in data.keys() if k[0] == \"F\" and k not in models]):\n",
    "\n",
    "    # get target dimensions\n",
    "    f_dim = data[var].shape[1]\n",
    "\n",
    "    # get input variables\n",
    "    g_var = \"G\" + var[1:]        \n",
    "    x_var = \"X\" + var[1:]\n",
    "    while x_var not in data:\n",
    "        x_num = int(x_var[1:]) - 1\n",
    "        if x_num == 0:\n",
    "            raise RuntimeError(\"Input data not loaded.\")\n",
    "        x_var = f\"X{x_num:02d}\"\n",
    "\n",
    "    # get training mask\n",
    "    mask = masks[x_var][\"train\"]\n",
    "\n",
    "    models[var] = {g_var: [], x_var: []}\n",
    "    for i in range(f_dim):\n",
    "\n",
    "        # get target data\n",
    "        y = data[var][mask,i]\n",
    "\n",
    "        # learn f(x)\n",
    "        print(f\"Learning {var}_{i}({g_var}).\")\n",
    "        X = data[g_var][mask]\n",
    "        models[var][g_var].append(get_model(X, y))\n",
    "\n",
    "        joblib.dump(models, os.path.join(model_path, model_name))\n",
    "    \n",
    "        # learn f(g(x))\n",
    "        print(f\"Learning {var}_{i}({x_var}).\")\n",
    "        X = data[x_var][mask]\n",
    "        models[var][x_var].append(get_model(X, y))\n",
    "\n",
    "        joblib.dump(models, os.path.join(model_path, model_name))\n",
    "\n",
    "    # get target dimensions\n",
    "    g_dim = data[g_var].shape[1]\n",
    "\n",
    "    models[g_var] = {x_var: []}\n",
    "    for i in range(g_dim):\n",
    "\n",
    "        # get target data\n",
    "        y = data[g_var][mask,i]\n",
    "   \n",
    "        # learn g(x)\n",
    "        print(f\"Learning {g_var}_{i}({x_var}).\")\n",
    "        X = data[x_var][mask]\n",
    "        models[g_var][x_var].append(get_model(X, y))\n",
    "\n",
    "        joblib.dump(models, os.path.join(model_path, model_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted hall_of_fame_2022-06-23_184611.830.csv.\n",
      "Deleted hall_of_fame_2022-06-23_184611.830.csv.bkup.\n",
      "Deleted hall_of_fame_2022-06-23_184921.625.csv.\n",
      "Deleted hall_of_fame_2022-06-23_184921.625.csv.bkup.\n",
      "Deleted hall_of_fame_2022-06-23_185027.264.csv.\n",
      "Deleted hall_of_fame_2022-06-23_185027.264.csv.bkup.\n",
      "Deleted hall_of_fame_2022-06-23_185141.449.csv.\n",
      "Deleted hall_of_fame_2022-06-23_185141.449.csv.bkup.\n",
      "Deleted hall_of_fame_2022-06-23_185236.513.csv.\n",
      "Deleted hall_of_fame_2022-06-23_185236.513.csv.bkup.\n",
      "Deleted hall_of_fame_2022-06-23_185355.318.csv.\n",
      "Deleted hall_of_fame_2022-06-23_185355.318.csv.bkup.\n",
      "Deleted hall_of_fame_2022-06-23_185458.617.csv.\n",
      "Deleted hall_of_fame_2022-06-23_185458.617.csv.bkup.\n",
      "Deleted hall_of_fame_2022-06-23_185603.693.csv.\n",
      "Deleted hall_of_fame_2022-06-23_185603.693.csv.bkup.\n",
      "Deleted hall_of_fame_2022-06-23_185657.080.csv.\n",
      "Deleted hall_of_fame_2022-06-23_185657.080.csv.bkup.\n",
      "Deleted hall_of_fame_2022-06-23_185745.349.csv.\n",
      "Deleted hall_of_fame_2022-06-23_185745.349.csv.bkup.\n",
      "Deleted hall_of_fame_2022-06-23_185855.922.csv.\n",
      "Deleted hall_of_fame_2022-06-23_185855.922.csv.bkup.\n",
      "Deleted hall_of_fame_2022-06-23_190003.163.csv.\n",
      "Deleted hall_of_fame_2022-06-23_190003.163.csv.bkup.\n",
      "Deleted hall_of_fame_2022-06-23_190045.476.csv.\n",
      "Deleted hall_of_fame_2022-06-23_190045.476.csv.bkup.\n",
      "Deleted hall_of_fame_2022-06-23_190143.460.csv.\n",
      "Deleted hall_of_fame_2022-06-23_190143.460.csv.bkup.\n",
      "Deleted hall_of_fame_2022-06-23_190256.616.csv.\n",
      "Deleted hall_of_fame_2022-06-23_190256.616.csv.bkup.\n",
      "Deleted hall_of_fame_2022-06-23_190357.652.csv.\n",
      "Deleted hall_of_fame_2022-06-23_190357.652.csv.bkup.\n",
      "Deleted hall_of_fame_2022-06-23_190454.845.csv.\n",
      "Deleted hall_of_fame_2022-06-23_190454.845.csv.bkup.\n",
      "Deleted hall_of_fame_2022-06-23_190551.244.csv.\n",
      "Deleted hall_of_fame_2022-06-23_190551.244.csv.bkup.\n",
      "Deleted hall_of_fame_2022-06-23_190658.908.csv.\n",
      "Deleted hall_of_fame_2022-06-23_190658.908.csv.bkup.\n",
      "Deleted hall_of_fame_2022-06-23_190732.664.csv.\n",
      "Deleted hall_of_fame_2022-06-23_190732.664.csv.bkup.\n",
      "Deleted hall_of_fame_2022-06-23_190938.331.csv.\n",
      "Deleted hall_of_fame_2022-06-23_190938.331.csv.bkup.\n"
     ]
    }
   ],
   "source": [
    "del_name = \"hall_of_fame_\"\n",
    "\n",
    "for f in os.listdir():\n",
    "    if del_name in f:\n",
    "        os.remove(f)\n",
    "        print(f\"Deleted {f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.3: Check discovery\n",
    "\n",
    "We check whether models were identified correctly based on the PySR loss. More advanced checks could consider the validation data or out-of-distribution data, as the correct model would both generalize and extrapolate accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_eps = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F01_0(G01): 9.65e-13 - [X] - (x0 * (x0 + 1.0000001))\n",
      "F01_0(X01): 6.57e+00 - [ ] - ((x0 * ((x0 * x0) - -3.4709718)) * (x0 + x1))\n",
      "G01_0(X01): 1.86e-14 - [X] - ((x0 * (x0 + x1)) + cos(x1))\n",
      "F02_0(G02): 5.83e-10 - [X] - (((x0 + x1) + x2) * (((x0 + 0.9999908) + x1) + x2))\n",
      "F02_0(X02): 1.91e+01 - [ ] - ((exp(x0 - -0.6688493) - exp(0.5662412 - x0)) * (x1 + x0))\n",
      "G02_0(X02): 9.49e-15 - [X] - (x0 * x0)\n",
      "G02_1(X02): 7.52e-16 - [X] - cos(x1)\n",
      "G02_2(X02): 2.82e-15 - [X] - (x1 * x0)\n",
      "F03_0(G03): 4.56e-11 - [X] - (x0 * (x0 + 2.745))\n",
      "F03_0(X03): 9.09e+02 - [ ] - exp(x2 * 2.6427264)\n",
      "G03_0(X03): 1.87e-13 - [X] - ((exp(x2) + ((x2 * x2) * x2)) + (sin(x0) * cos(x1)))\n",
      "F04_0(G04): 8.81e-13 - [X] - ((x1 * x2) + (x0 * x0))\n",
      "F04_0(X04): 8.51e+00 - [ ] - exp(x2 * 2.0940368)\n",
      "G04_0(X04): 7.09e-16 - [X] - (cos(x1) * sin(x0))\n",
      "G04_1(X04): 8.45e-14 - [X] - (x2 * (x2 * x2))\n",
      "G04_2(X04): 1.41e-14 - [X] - exp(x2)\n",
      "F05_0(G05): 1.57e+03 - [ ] - exp(x1 * 0.68942153)\n",
      "F05_0(X05): 3.60e+03 - [ ] - exp(((x1 * x3) * 1.6188184) - x2)\n",
      "G05_0(X05): 0.00e+00 - [X] - x0\n",
      "G05_1(X05): 1.18e-14 - [X] - (x1 * (x3 * x3))\n",
      "G05_2(X05): 8.05e-11 - [X] - log_abs(x3 * -2.0000145)\n"
     ]
    }
   ],
   "source": [
    "for d_var in models:\n",
    "    for i_var in models[d_var]:\n",
    "        for m, model in enumerate(models[d_var][i_var]):\n",
    "            best = model.get_best()\n",
    "            print(f\"{d_var}_{m}({i_var}): {best.loss:.2e} - [{(' ','X')[best.loss < disc_eps]}] - {best.equation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Only `F05_0(G05)` does not work as intended."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "540f5367292c701f747780e6da702d3852f1c7c25c6067d18f36ab6562a2dcf2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
