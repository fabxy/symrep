{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2022 Flatiron Machine Learning x Science Summer School\n",
    "\n",
    "## Step 2: Run symbolic regression on generic data\n",
    "\n",
    "In this step, we investigate whether symbolic regression can discover the functions underlying the generic data created in Step 1. There is no support from deep learning at this point.\n",
    "\n",
    "We use the symbolic regression package [PySR](https://github.com/MilesCranmer/PySR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "from pysr import PySRRegressor\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.1: Load data\n",
    "\n",
    "We load the input and target function data created in Step 1 and create masks to split the data into training, validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data\"\n",
    "data_ext = \".gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded F01 data.\n",
      "Loaded F02 data.\n",
      "Loaded F03 data.\n",
      "Loaded F04 data.\n",
      "Loaded G01 data.\n",
      "Loaded G02 data.\n",
      "Loaded G03 data.\n",
      "Loaded G04 data.\n",
      "Loaded X01 data.\n",
      "Loaded X04 data.\n"
     ]
    }
   ],
   "source": [
    "data = {}\n",
    "for file_name in os.listdir(data_path):\n",
    "    if file_name[-len(data_ext):] == data_ext:\n",
    "        \n",
    "        var = file_name[:-len(data_ext)]\n",
    "        var_data = np.loadtxt(os.path.join(data_path, file_name))\n",
    "        if len(var_data.shape) == 1:\n",
    "            var_data = var_data.reshape(-1,1)\n",
    "        data[var] = var_data\n",
    "\n",
    "        print(f\"Loaded {var} data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "\n",
    "train_size = 0.7\n",
    "val_size = 0.2\n",
    "\n",
    "mask_ext = \".mask\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded masks for X01 data.\n",
      "Created masks for X04 data.\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "\n",
    "masks = {}\n",
    "for var in data:\n",
    "    if var[0] == \"X\":\n",
    "\n",
    "        mask_name = var + mask_ext\n",
    "        try:\n",
    "            masks[var] = joblib.load(os.path.join(data_path, mask_name))\n",
    "            print(f\"Loaded masks for {var} data.\")\n",
    "        except:\n",
    "            data_size = data[var].shape[0]\n",
    "\n",
    "            data_idx = np.arange(data_size)\n",
    "            np.random.shuffle(data_idx)\n",
    "\n",
    "            train_idx = int(data_size*train_size)\n",
    "            val_idx = train_idx + int(data_size*val_size)\n",
    "\n",
    "            masks[var] = {\n",
    "                \"train\": data_idx[:train_idx],\n",
    "                \"val\": data_idx[train_idx:val_idx],\n",
    "                \"test\": data_idx[val_idx:],\n",
    "            }\n",
    "    \n",
    "            joblib.dump(masks[var], os.path.join(data_path, mask_name))\n",
    "\n",
    "            print(f\"Created masks for {var} data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.2: Run PySR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(X, y):\n",
    "\n",
    "    model = PySRRegressor(\n",
    "        procs=4,\n",
    "        populations=30,\n",
    "        niterations=30,\n",
    "        maxsize=20,\n",
    "        binary_operators=[\"plus\", \"sub\", \"mult\"],\n",
    "        unary_operators=[\"sin\", \"cos\", \"exp\", \"log_abs\"],      \n",
    "        model_selection=\"best\",\n",
    "        verbosity=0\n",
    "    )\n",
    "\n",
    "    model.fit(X, y)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"models\"\n",
    "model_name = \"pysr_models_v1.pkl\"\n",
    "\n",
    "try:\n",
    "    models = joblib.load(os.path.join(model_path, model_name))\n",
    "except:\n",
    "    models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in sorted([k for k in data.keys() if k[0] == \"F\" and k not in models]):\n",
    "\n",
    "    # get target dimensions\n",
    "    f_dim = data[var].shape[1]\n",
    "\n",
    "    # get input variables\n",
    "    g_var = \"G\" + var[1:]        \n",
    "    x_var = \"X\" + var[1:]\n",
    "    while x_var not in data:\n",
    "        x_num = int(x_var[1:]) - 1\n",
    "        if x_num == 0:\n",
    "            raise RuntimeError(\"Input data not loaded.\")\n",
    "        x_var = f\"X{x_num:02d}\"\n",
    "\n",
    "    # get training mask\n",
    "    mask = masks[x_var][\"train\"]\n",
    "\n",
    "    models[var] = {g_var: [], x_var: []}\n",
    "    for i in range(f_dim):\n",
    "\n",
    "        # get target data\n",
    "        y = data[var][mask,i]\n",
    "\n",
    "        # learn f(x)\n",
    "        print(f\"Learning {var}_{i}({g_var}).\")\n",
    "        X = data[g_var][mask]\n",
    "        models[var][g_var].append(get_model(X, y))\n",
    "\n",
    "        joblib.dump(models, os.path.join(model_path, model_name))\n",
    "    \n",
    "        # learn f(g(x))\n",
    "        print(f\"Learning {var}_{i}({x_var}).\")\n",
    "        X = data[x_var][mask]\n",
    "        models[var][x_var].append(get_model(X, y))\n",
    "\n",
    "        joblib.dump(models, os.path.join(model_path, model_name))\n",
    "\n",
    "    # get target dimensions\n",
    "    g_dim = data[g_var].shape[1]\n",
    "\n",
    "    models[g_var] = {x_var: []}\n",
    "    for i in range(g_dim):\n",
    "\n",
    "        # get target data\n",
    "        y = data[g_var][mask,i]\n",
    "   \n",
    "        # learn g(x)\n",
    "        print(f\"Learning {g_var}_{i}({x_var}).\")\n",
    "        X = data[x_var][mask]\n",
    "        models[g_var][x_var].append(get_model(X, y))\n",
    "\n",
    "        joblib.dump(models, os.path.join(model_path, model_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_name = \"hall_of_fame_\"\n",
    "\n",
    "for f in os.listdir():\n",
    "    if del_name in f:\n",
    "        os.remove(f)\n",
    "        print(f\"Deleted {f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.3: Check discovery\n",
    "\n",
    "We check whether models were identified correctly based on the PySR loss. More advanced checks could consider the validation data or out-of-distribution data, as the correct model would both generalize and extrapolate accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_eps = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d_var in models:\n",
    "    for i_var in models[d_var]:\n",
    "        for m, model in enumerate(models[d_var][i_var]):\n",
    "            best = model.get_best()\n",
    "            print(f\"{d_var}_{m}({i_var}): {best.loss:.2e} - [{(' ','X')[best.loss < disc_eps]}] - {best.equation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: The discovered model `G03_0(X01)` contains one too many terms (`-1.0469586e-5 * x3`)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "540f5367292c701f747780e6da702d3852f1c7c25c6067d18f36ab6562a2dcf2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
