{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2022 Flatiron Machine Learning x Science Summer School\n",
    "\n",
    "## Step 2: Run symbolic regression on generic data\n",
    "\n",
    "In this step, we investigate whether symbolic regression can discover the functions underlying the generic data created in Step 1. There is no support from deep learning at this point.\n",
    "\n",
    "We use the symbolic regression package [PySR](https://github.com/MilesCranmer/PySR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "from pysr import PySRRegressor\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.1: Load data\n",
    "\n",
    "We load the input and target function data created in Step 1 and create masks to split the data into training, validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data\"\n",
    "data_ext = \".gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded F01 data.\n",
      "Loaded F02 data.\n",
      "Loaded F03 data.\n",
      "Loaded G01 data.\n",
      "Loaded G02 data.\n",
      "Loaded G03 data.\n",
      "Loaded X01 data.\n"
     ]
    }
   ],
   "source": [
    "data = {}\n",
    "for file_name in os.listdir(data_path):\n",
    "    if file_name[-len(data_ext):] == data_ext:\n",
    "        \n",
    "        var = file_name[:-len(data_ext)]\n",
    "        var_data = np.loadtxt(os.path.join(data_path, file_name))\n",
    "        if len(var_data.shape) == 1:\n",
    "            var_data = var_data.reshape(-1,1)\n",
    "        data[var] = var_data\n",
    "\n",
    "        print(f\"Loaded {var} data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "\n",
    "train_size = 0.7\n",
    "val_size = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created masks for X01 data.\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "train_mask = {}\n",
    "val_mask = {}\n",
    "test_mask = {}\n",
    "\n",
    "for var in data:\n",
    "    if var[0] == \"X\":\n",
    "        data_size = data[var].shape[0]\n",
    "\n",
    "        data_idx = np.arange(data_size)\n",
    "        np.random.shuffle(data_idx)\n",
    "\n",
    "        train_idx = int(data_size*train_size)\n",
    "        val_idx = train_idx + int(data_size*val_size)\n",
    "\n",
    "        train_mask[var] = data_idx[:train_idx]\n",
    "        val_mask[var] = data_idx[train_idx:val_idx]\n",
    "        test_mask[var] = data_idx[val_idx:]\n",
    "\n",
    "        print(f\"Created masks for {var} data.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.2: Run PySR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(X, y):\n",
    "\n",
    "    model = PySRRegressor(\n",
    "        procs=4,\n",
    "        populations=30,\n",
    "        niterations=30,\n",
    "        maxsize=20,\n",
    "        binary_operators=[\"plus\", \"sub\", \"mult\"],\n",
    "        unary_operators=[\"sin\", \"cos\", \"exp\", \"log_abs\"],      \n",
    "        model_selection=\"best\",\n",
    "        verbosity=0\n",
    "    )\n",
    "\n",
    "    model.fit(X, y)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"models\"\n",
    "model_name = \"pysr_models_v1.pkl\"\n",
    "\n",
    "try:\n",
    "    models = joblib.load(os.path.join(model_path, model_name))\n",
    "except:\n",
    "    models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning F03_0(G03).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: ignoring conflicting import of CoreModule.div into SymbolicRegression\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning F03_0(X01).\n",
      "Learning G03_0(X01).\n"
     ]
    }
   ],
   "source": [
    "for var in sorted([k for k in data.keys() if k[0] == \"F\" and k not in models]):\n",
    "\n",
    "    # get target dimensions\n",
    "    f_dim = data[var].shape[1]\n",
    "\n",
    "    # get input variables\n",
    "    g_var = \"G\" + var[1:]        \n",
    "    x_var = \"X\" + var[1:]\n",
    "    while x_var not in data:\n",
    "        x_num = int(x_var[1:]) - 1\n",
    "        if x_num == 0:\n",
    "            raise RuntimeError(\"Input data not loaded.\")\n",
    "        x_var = f\"X{x_num:02d}\"\n",
    "\n",
    "    # get training mask\n",
    "    mask = train_mask[x_var]\n",
    "\n",
    "    models[var] = {g_var: [], x_var: []}\n",
    "    for i in range(f_dim):\n",
    "\n",
    "        # get target data\n",
    "        y = data[var][mask,i]\n",
    "\n",
    "        # learn f(x)\n",
    "        print(f\"Learning {var}_{i}({g_var}).\")\n",
    "        X = data[g_var][mask]\n",
    "        models[var][g_var].append(get_model(X, y))\n",
    "\n",
    "        joblib.dump(models, os.path.join(model_path, model_name))\n",
    "    \n",
    "        # learn f(g(x))\n",
    "        print(f\"Learning {var}_{i}({x_var}).\")\n",
    "        X = data[x_var][mask]\n",
    "        models[var][x_var].append(get_model(X, y))\n",
    "\n",
    "        joblib.dump(models, os.path.join(model_path, model_name))\n",
    "\n",
    "    # get target dimensions\n",
    "    g_dim = data[g_var].shape[1]\n",
    "\n",
    "    models[g_var] = {x_var: []}\n",
    "    for i in range(g_dim):\n",
    "\n",
    "        # get target data\n",
    "        y = data[g_var][mask,i]\n",
    "   \n",
    "        # learn g(x)\n",
    "        print(f\"Learning {g_var}_{i}({x_var}).\")\n",
    "        X = data[x_var][mask]\n",
    "        models[g_var][x_var].append(get_model(X, y))\n",
    "\n",
    "        joblib.dump(models, os.path.join(model_path, model_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted hall_of_fame_2022-06-21_182314.694.csv.\n",
      "Deleted hall_of_fame_2022-06-21_182314.694.csv.bkup.\n",
      "Deleted hall_of_fame_2022-06-21_182703.562.csv.\n",
      "Deleted hall_of_fame_2022-06-21_182703.562.csv.bkup.\n",
      "Deleted hall_of_fame_2022-06-21_182824.482.csv.\n",
      "Deleted hall_of_fame_2022-06-21_182824.482.csv.bkup.\n"
     ]
    }
   ],
   "source": [
    "del_name = \"hall_of_fame_\"\n",
    "\n",
    "for f in os.listdir():\n",
    "    if del_name in f:\n",
    "        os.remove(f)\n",
    "        print(f\"Deleted {f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.3: Check discovery\n",
    "\n",
    "We check whether models were identified correctly based on the PySR loss. More advanced checks could consider the validation data or out-of-distribution data, as the correct model would both generalize and extrapolate accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_eps = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F01_0(G01): 4.23e-13 - [X] - (x0 * x0)\n",
      "F01_0(X01): 5.55e+01 - [ ] - exp(log_abs(x7) * 4.0997605)\n",
      "G01_0(X01): 2.37e-14 - [X] - ((((x7 * x7) + cos(x2)) + x0) + (x6 * x3))\n",
      "F02_0(G02): 1.87e-10 - [X] - (x0 * ((x0 * 3.9499998) + 1.0000001))\n",
      "F02_0(X01): 8.08e+03 - [ ] - (exp(exp(log_abs(x5) + 0.96497315)) + exp(x9 * 2.5191934))\n",
      "G02_0(X01): 1.61e-13 - [X] - (((x5 * (x5 * x5)) + exp(x9)) + (sin(x0) * cos(x1)))\n",
      "F03_0(G03): 3.29e-09 - [X] - (exp(x0 * 0.2499991) * x0)\n",
      "F03_0(X01): 1.24e+01 - [ ] - exp(((x5 * -0.5841311) * x3) * x5)\n",
      "G03_0(X01): 1.15e-09 - [X] - ((log_abs(x8 + x7) + (x3 * (-1.0469586e-5 - exp(log_abs(x5) * 1.9999653)))) + x0)\n"
     ]
    }
   ],
   "source": [
    "for d_var in models:\n",
    "    for i_var in models[d_var]:\n",
    "        for m, model in enumerate(models[d_var][i_var]):\n",
    "            best = model.get_best()\n",
    "            print(f\"{d_var}_{m}({i_var}): {best.loss:.2e} - [{(' ','X')[best.loss < disc_eps]}] - {best.equation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: The discovered model `G03_0(X01)` contains one too many terms (`-1.0469586e-5 * x3`)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "540f5367292c701f747780e6da702d3852f1c7c25c6067d18f36ab6562a2dcf2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
