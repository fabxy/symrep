{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2022 Flatiron Machine Learning x Science Summer School\n",
    "\n",
    "This notebook trains SRNets on Colab and explains the hyperparameters.\n",
    "\n",
    "1. Clone `symrep` repo and install missing libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/fabxy/symrep.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd symrep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wandb einops --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. If you want to use `wandb`, login with your API key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import joblib\n",
    "from srnet import SRNet, SRData, run_training\n",
    "from sdnet import SDNet, SDData\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. If you want to use `wandb`, specify the project name and if applicable, the sweep ID and the number of sweep runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# set wandb options\n",
    "wandb_project = \"132-bn-mask-DSN-sd-study-F00_v5\"\n",
    "sweep_id = None\n",
    "sweep_num = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Load the training and validation data. If you want to generate new data, the format is the output format of `np.savetxt`, which is a space-separated text file without headers. Additionally, a pickled dictionary is loaded, which contains masks (`np.array`) to select the training, validation and testing data from the complete data file, e.g. `F00.gz`. Alternatively, you can save the training, validation and testing data into separate folders and not pass masks to `SRData`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data_path = \"data_1k\"\n",
    "\n",
    "in_var = \"X00\"\n",
    "lat_var = None         # the true latent feature values, not required for training\n",
    "target_var = \"F00\"\n",
    "\n",
    "mask_ext = \".mask\"\n",
    "masks = joblib.load(os.path.join(data_path, in_var + mask_ext))\n",
    "\n",
    "train_data = SRData(data_path, in_var, lat_var, target_var, masks[\"train\"], device=device)\n",
    "val_data = SRData(data_path, in_var, lat_var, target_var, masks[\"val\"], device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Load the symbolic discriminator (SD) function library. The `shuffle` determines if the order of the loaded functions is shuffled, which should be true when there are more functions in the library than the SD loads per iteration. If the input data, i.e. `train_data.in_data`, is passed to `SDData`, all function values are computed before the training (otherwise, these values are computed during training). Furthermore, each character `U` and `N` in any function string is replaced during evaluation with a scalar value sampled from a uniform (between 0 and 1) and a normal (mean 0 and variance 1) distribution, respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create discriminator data\n",
    "fun_path = \"funs/F00_v5.lib\"\n",
    "shuffle = True\n",
    "\n",
    "if fun_path:\n",
    "    disc_data = SDData(fun_path, in_var, shuffle=shuffle)\n",
    "else:\n",
    "    disc_data = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Define load and save `pkl` file names and the logging frequency. Setting a load file allows restarting the training. If a `wandb` project is defined, the save file is also saved to `wandb`. The logging frequency determines the update frequency of `tqdm`, `wandb` and the size of the save file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set load and save file\n",
    "load_file = None\n",
    "save_file = \"models/srnet_model_F00_v5_bn_mask_sd_study.pkl\"\n",
    "log_freq = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Set the hyperparameters which define the `SRNet` and `SDNet` architectures and the training.\n",
    "\n",
    "* `arch` define the `SRNet` architecture:\n",
    "\n",
    "    * `in_size` and `out_size` are determined by the training data\n",
    "    \n",
    "    * `hid_num` and `hid_size` define the hidden layers:\n",
    "    \n",
    "        * Note that the general structure of the network is `input -> layers1 -> latent space -> layers2 -> output`\n",
    "        \n",
    "        * If `hid_num` or `hid_size` are scalar values, they are applied to both `layers1` and `layers2`\n",
    "        \n",
    "        * If `hid_num` or `hid_size` are tuples, the first values are applied to `layers1` and the second values to `layers2`\n",
    "        \n",
    "        * Let's call this tuple definition logic (TDL)\n",
    "        \n",
    "    * `hid_type` allows to select between a disentangled sparsity network (`DSN`) and a multi-layer perceptron (`MLP`) (TDL applies)\n",
    "    \n",
    "    * `hid_kwargs` allows passing additional keyword arguments to `layers1` and `layers2` (TDL applies):\n",
    "    \n",
    "        * `alpha` controls the input mask of the DSN. If `None`, the parameters are learned. If `-1`, the parameters are randomly initialized, but not learned (useful when using pre-trained network). If specific values are defined, these are set and no learning occurs. Since we have previously shown that learning the correct input dependencies is feasible, we fix the input mask to simplify the learning problem.\n",
    "        \n",
    "        * `norm` is the identity function for `None` and applies softmax for `softmax`\n",
    "        \n",
    "        * `prune` sets input mask parameters that are below the defined pruning value to zero\n",
    "        \n",
    "    * `lat_size` defines the number of latent space nodes. The value is set to the target value here, as it was previously shown that it can be determined correctly.\n",
    "    \n",
    "* `epochs` defines the number of epochs\n",
    "\n",
    "* `runtime` allows defining a maximum runtime in seconds, after which the training is stopped\n",
    "\n",
    "* `batch_size` determines the batch size and the current value uses all training data points in one batch\n",
    "\n",
    "* `shuffle` determines whether the training data points are shuffled during training\n",
    "\n",
    "* `lr` denotes the learning rate of `SRNet`\n",
    "\n",
    "The following parameters define various types of regularization:\n",
    "\n",
    "* `wd` denotes the weight decay\n",
    "\n",
    "* `l1` applies L1 regularization on the latent feature activations\n",
    "\n",
    "* `a1` and `a2` regularize the DSN input mask according to https://astroautomata.com/data/sjnn_paper.pdf\n",
    "\n",
    "* `e1` regularizes the row-wise entropy of the normalized DSN input mask\n",
    "\n",
    "* `e2` is an experimental approach to regularize a combination of entropy and variance\n",
    "\n",
    "* `e3` is an experimental approach to regularize correlations between latent features and SD library functions\n",
    "\n",
    "* `sd` weights the real vs. fake predictions of the SD for the latent feature activations\n",
    "\n",
    "* `disc` describes the SD architecture:\n",
    "\n",
    "    * `hid_num` and `hid_size` define the hidden layers of the MLP\n",
    "    \n",
    "    * If `emb_size` is not `None`, a second MLP is created which embeds multiple dimensions, e.g. input dimensions and target value (if any other information should be embedded, please modify `run_training` code), into a single dimension, which is then passed to the actual SD. In this case, TDL applies to `hid_num` and `hid_size`.\n",
    "    \n",
    "    * `lr` defines the SD learning rate\n",
    "    \n",
    "    * `wd` defines the SD weight decay\n",
    "    \n",
    "    * `iters` defines the number of iterations of SD training per `SRNet` (\"generator\") iteration\n",
    "    \n",
    "    * `gp` defines the level of gradient penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters\n",
    "hyperparams = {\n",
    "    \"arch\": {\n",
    "        \"in_size\": train_data.in_data.shape[1],\n",
    "        \"out_size\": train_data.target_data.shape[1],\n",
    "        \"hid_num\": (2,0),\n",
    "        \"hid_size\": 32, \n",
    "        \"hid_type\": (\"DSN\", \"MLP\"),\n",
    "        \"hid_kwargs\": {\n",
    "            \"alpha\": [[1,0],[0,1],[1,1]],\n",
    "            \"norm\": None,\n",
    "            \"prune\": None,\n",
    "            },\n",
    "        \"lat_size\": 3,\n",
    "        },\n",
    "    \"epochs\": 30000,\n",
    "    \"runtime\": None,\n",
    "    \"batch_size\": train_data.in_data.shape[0],\n",
    "    \"shuffle\": False,\n",
    "    \"lr\": 1e-4,\n",
    "    \"wd\": 1e-6,\n",
    "    \"l1\": 0.0,\n",
    "    \"a1\": 0.0,\n",
    "    \"a2\": 0.0,\n",
    "    \"e1\": 0.0,\n",
    "    \"e2\": 0.0,\n",
    "    \"e3\": 0.0,\n",
    "    \"gc\": 0.0,\n",
    "    \"sd\": 1e-6,\n",
    "    \"disc\": {\n",
    "        \"hid_num\": 6,\n",
    "        \"hid_size\": 128,\n",
    "        \"emb_size\": None,\n",
    "        \"lr\": 1e-3,\n",
    "        \"wd\": 1e-4,\n",
    "        \"iters\": 5,\n",
    "        \"gp\": 1e-5,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Start training or sweep if `sweep_id` is defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    run_training(SRNet, hyperparams, train_data, val_data, SDNet, disc_data, load_file=load_file, save_file=save_file, log_freq=log_freq, device=device, wandb_project=wandb_project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter study\n",
    "if sweep_id:\n",
    "    wandb.agent(sweep_id, train, count=sweep_num, project=wandb_project)\n",
    "\n",
    "# one training run\n",
    "else:\n",
    "    train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "540f5367292c701f747780e6da702d3852f1c7c25c6067d18f36ab6562a2dcf2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
