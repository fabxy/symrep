{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2022 Flatiron Machine Learning x Science Summer School\n",
    "\n",
    "## Step 3: Train plain MLP\n",
    "\n",
    "In this step, we train plain multilayer perceptrons (MLP) to approximate the generic data of the various functions $f \\circ g$ created in Step 1.\n",
    "\n",
    "We set up the training pipeline and explore hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.1: Check convergence\n",
    "\n",
    "First, we define baseline hyperparameters and inspect the convergence of the resulting baseline models on the (so far) five datasets of generic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "from srnet import SRNet, SRData\n",
    "import srnet_utils as ut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set wandb project\n",
    "wandb_project = \"31-check-convergence\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters\n",
    "# hyperparams = {\n",
    "#     \"arch\": {\n",
    "#         \"in_size\": train_data.in_data.shape[1],\n",
    "#         \"out_size\": train_data.target_data.shape[1],\n",
    "#         \"hid_num\": 2,\n",
    "#         \"hid_size\": 32, \n",
    "#         \"hid_type\": \"MLP\",\n",
    "#         \"lat_size\": 16,\n",
    "#         },\n",
    "#     \"epochs\": 10000,\n",
    "#     \"runtime\": None,\n",
    "#     \"batch_size\": 64,\n",
    "#     \"lr\": 1e-4,\n",
    "#     \"wd\": 1e-4,\n",
    "#     # \"l1\": 1e-4,\n",
    "#     \"shuffle\": True,\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading srnet_model_F06_conv1k.pkl.\n",
      "Downloading srnet_model_F05_conv.pkl.\n",
      "Downloading srnet_model_F04_conv.pkl.\n",
      "Downloading srnet_model_F03_conv.pkl.\n",
      "Downloading srnet_model_F02_conv.pkl.\n",
      "Downloading srnet_model_F01_conv.pkl.\n"
     ]
    }
   ],
   "source": [
    "# download data from wandb\n",
    "file_ext = \".pkl\"\n",
    "\n",
    "api = wandb.Api()\n",
    "\n",
    "runs = api.runs(wandb_project)\n",
    "for run in runs:\n",
    "    for f in run.files():\n",
    "        if f.name[-len(file_ext):] == file_ext and not os.path.isfile(f.name):\n",
    "            print(f\"Downloading {os.path.basename(f.name)}.\")\n",
    "            run.file(f.name).download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39fca914ef02404faef2b46d06e78f05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['srnet_model_F00_conv1k',\n",
       " 'srnet_model_F01_conv1k',\n",
       " 'srnet_model_F02_conv1k',\n",
       " 'srnet_model_F03_conv1k',\n",
       " 'srnet_model_F04_conv1k',\n",
       " 'srnet_model_F05_conv1k',\n",
       " 'srnet_model_F06_conv1k']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plot losses\n",
    "ut.plot_losses(\"conv1k\", save_path=\"models\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "* `F01` and `F05` seem well converged with a reasonable validation loss\n",
    "\n",
    "* `F02` has a significantly higher validation loss than `F01`, despite having the same underlying target function (however, `X01` and `X02` are different)\n",
    "\n",
    "* `F04` also has a significant validation loss that oscillates\n",
    "\n",
    "* `F03` shows a massive validation loss\n",
    "\n",
    "* `F00` as the simplest expression shows the lowest training and validation errors (however, some overfitting seems to occur)\n",
    "\n",
    "* `F06` has a low training error, but the validation loss is not very low. Are we overfitting?\n",
    "\n",
    "Analyzing the models for all datasets and optimizing their hyperparameters might be difficult. \n",
    "\n",
    "And how do we know that $g$ is approximated by the first part and $f$ by the second part of the network?\n",
    "\n",
    "Let's start with `F00`.\n",
    "\n",
    "### Step 3.2: Analyze `F00` model\n",
    "\n",
    "Let's check how well the current model for `F00` performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data_path = \"data_1k\"\n",
    "\n",
    "in_var = \"X00\"\n",
    "lat_var = \"G00\"\n",
    "target_var = \"F00\"\n",
    "\n",
    "mask_ext = \".mask\"\n",
    "masks = joblib.load(os.path.join(data_path, in_var + mask_ext))     # TODO: create mask if file does not exist\n",
    "\n",
    "train_data = SRData(data_path, in_var, lat_var, target_var, masks[\"train\"])\n",
    "val_data = SRData(data_path, in_var, lat_var, target_var, masks[\"val\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model_name = \"srnet_model_F00_conv1k.pkl\"\n",
    "model_path = \"models\"\n",
    "\n",
    "model = ut.load_model(model_name, model_path, SRNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions\n",
    "with torch.no_grad():\n",
    "    preds = model(train_data.in_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select plotting data\n",
    "x_data = train_data.in_data[:,0]\n",
    "y_data = train_data.in_data[:,1]\n",
    "z_data = [(\"target\", train_data.target_data), (\"pred\", preds)]\n",
    "plot_size = train_data.target_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut.plot_acts(x_data, y_data, z_data, plot_size=plot_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions on the training data seem to be good enough. What about the validation data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions\n",
    "with torch.no_grad():\n",
    "    preds = model(val_data.in_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select plotting data\n",
    "x_data = val_data.in_data[:,0]\n",
    "y_data = val_data.in_data[:,1]\n",
    "z_data = [(\"target\", val_data.target_data), (\"pred\", preds)]\n",
    "plot_size = val_data.target_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut.plot_acts(x_data, y_data, z_data, plot_size=plot_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Except for a few outliers at the edges, the validation data is also approximated accurately.\n",
    "\n",
    "**TODO**: Rerun the pipeline with a dataset size of 10,000.\n",
    "\n",
    "What do the **latent features** look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions\n",
    "with torch.no_grad():\n",
    "    preds, acts = model(train_data.in_data, get_lat=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get latent feature variance\n",
    "all_nodes = ut.get_node_order(acts, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = all_nodes[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select plotting data\n",
    "x_data = train_data.in_data[:,0]\n",
    "y_data = train_data.in_data[:,1]\n",
    "z_data = [\n",
    "    (\"target\", train_data.target_data),\n",
    "    #(\"x**2\", train_data.lat_data[:,0]), \n",
    "    #(\"cos(y)\", train_data.lat_data[:,1]), \n",
    "    #(\"x*y\", train_data.lat_data[:,2]),\n",
    "]\n",
    "plot_size = train_data.target_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut.plot_acts(x_data, y_data, z_data, acts=acts, nodes=nodes, model=model, agg=True, plot_size=plot_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.3: Optimize hyperparameters for `F00`\n",
    "\n",
    "**STILL OPEN**\n",
    "\n",
    "We investigate the effects of the following hyperparameters:\n",
    "\n",
    "* Architecture (`hid_num`, `hid_size`, `lat_size`)\n",
    "\n",
    "* Dataset size\n",
    "\n",
    "* Batch size\n",
    "\n",
    "* Learning rate\n",
    "\n",
    "* Weight decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    \"method\": \"random\", # grid, bayesian\n",
    "    \"metric\": {\n",
    "        \"name\": \"val_loss\",\n",
    "        \"goal\": \"minimize\",\n",
    "    },\n",
    "    \"lr\": {\n",
    "        \"values\": [1e-3, 5e-4, 1e-4]\n",
    "    },\n",
    "    \"batch_size\": {\n",
    "        \"values\": [16, 32, 64]\n",
    "    },\n",
    "    \"hid_num1\": {\n",
    "        \"values\": [2, 4, 8]\n",
    "    },\n",
    "    \"hid_num2\": {\n",
    "        \"values\": [1, 2, 4]\n",
    "    },\n",
    "    \"hid_size\": {\n",
    "        \"values\": [32, 64, 128]\n",
    "    },\n",
    "}\n",
    "\n",
    "sweep_config[\"parameters\"] = parameters_dict\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes**:\n",
    "\n",
    "Running the training on a GPU is actually slower than on a CPU. (19.26it/s vs. 31.04it/s)\n",
    "\n",
    "`wandb.watch` slows down the training process\n",
    "\n",
    "`num_workers` also slows down the training process (7.38it/s vs. 17.88it/s for `num_workers=2`)\n",
    "\n",
    "`torch.backends.cudnn.benchmark` does not impact the GPU training speed\n",
    "\n",
    "\n",
    "Check:\n",
    "\n",
    "* `accelerate`\n",
    "\n",
    "* `lighting`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.4: Analyze `F06` model\n",
    "\n",
    "Due to 8 input features, the latent features cannot be plotted that easily anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data_path = \"data_1k\"\n",
    "\n",
    "in_var = \"X06\"\n",
    "lat_var = \"G06\"\n",
    "target_var = \"F06\"\n",
    "\n",
    "mask_ext = \".mask\"\n",
    "masks = joblib.load(os.path.join(data_path, in_var + mask_ext))     # TODO: create mask if file does not exist\n",
    "\n",
    "train_data = SRData(data_path, in_var, lat_var, target_var, masks[\"train\"])\n",
    "val_data = SRData(data_path, in_var, lat_var, target_var, masks[\"val\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model_name = \"srnet_model_F06_conv1k.pkl\"\n",
    "model_path = \"models\"\n",
    "\n",
    "model = ut.load_model(model_name, model_path, SRNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions\n",
    "with torch.no_grad():\n",
    "    preds, acts = model(train_data.in_data, get_lat=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.07345882, 0.038932025, 0.038396627, 0.030609325, 0.022553695, 0.021590559, 0.021060195, 0.018421583, 0.016839076, 0.014656181, 0.013953692, 0.013143861, 0.012904241, 0.011364459, 0.008470869, 0.0072448477]\n",
      "[4, 5, 2, 10, 7, 0, 13, 12, 3, 14, 9, 1, 6, 11, 15, 8]\n"
     ]
    }
   ],
   "source": [
    "# get latent feature variance\n",
    "all_nodes = ut.get_node_order(acts, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = all_nodes[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select data\n",
    "x0_data = train_data.in_data[:,0]\n",
    "x3_data = train_data.in_data[:,3]\n",
    "x5_data = train_data.in_data[:,5]\n",
    "x7_data = train_data.in_data[:,7]\n",
    "\n",
    "corr_data = [\n",
    "    (\"x0**2\", x0_data**2), \n",
    "    (\"cos(x3)\", np.cos(x3_data)), \n",
    "    (\"x5*x7\", x5_data * x7_data),\n",
    "    (\"x0\", x0_data),\n",
    "    (\"x3\", x3_data),\n",
    "    (\"x5\", x5_data),\n",
    "    (\"x7\", x7_data),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cad6a4d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Node 4\n",
      "corr(n4, x0**2): 0.6778/0.6778\n",
      "corr(n4, cos(x3)): -0.1845/-0.1845\n",
      "corr(n4, x5*x7): 0.2032/0.2032\n",
      "corr(n4, x0): -0.0778/-0.0778\n",
      "corr(n4, x3): -0.0098/-0.0098\n",
      "corr(n4, x5): 0.0490/0.0490\n",
      "corr(n4, x7): -0.0724/-0.0724\n",
      "\n",
      "Node 5\n",
      "corr(n5, x0**2): -0.7653/-0.7653\n",
      "corr(n5, cos(x3)): 0.0262/0.0262\n",
      "corr(n5, x5*x7): -0.3672/-0.3672\n",
      "corr(n5, x0): 0.1303/0.1303\n",
      "corr(n5, x3): 0.0474/0.0474\n",
      "corr(n5, x5): -0.0803/-0.0803\n",
      "corr(n5, x7): -0.1144/-0.1144\n",
      "\n",
      "Node 2\n",
      "corr(n2, x0**2): -0.8322/-0.8322\n",
      "corr(n2, cos(x3)): -0.1152/-0.1152\n",
      "corr(n2, x5*x7): -0.4042/-0.4042\n",
      "corr(n2, x0): -0.0667/-0.0667\n",
      "corr(n2, x3): -0.0849/-0.0849\n",
      "corr(n2, x5): -0.0361/-0.0361\n",
      "corr(n2, x7): 0.0767/0.0767\n",
      "\n",
      "Node 10\n",
      "corr(n10, x0**2): 0.5816/0.5816\n",
      "corr(n10, cos(x3)): -0.1766/-0.1766\n",
      "corr(n10, x5*x7): -0.0120/-0.0120\n",
      "corr(n10, x0): 0.1843/0.1843\n",
      "corr(n10, x3): 0.0886/0.0886\n",
      "corr(n10, x5): -0.0155/-0.0155\n",
      "corr(n10, x7): -0.0887/-0.0887\n",
      "\n",
      "Node 7\n",
      "corr(n7, x0**2): 0.8282/0.8282\n",
      "corr(n7, cos(x3)): 0.0887/0.0887\n",
      "corr(n7, x5*x7): 0.4154/0.4154\n",
      "corr(n7, x0): 0.0469/0.0469\n",
      "corr(n7, x3): 0.0410/0.0410\n",
      "corr(n7, x5): -0.0476/-0.0476\n",
      "corr(n7, x7): 0.0310/0.0310\n",
      "\n",
      "Node 0\n",
      "corr(n0, x0**2): 0.8034/0.8034\n",
      "corr(n0, cos(x3)): 0.1081/0.1081\n",
      "corr(n0, x5*x7): 0.4546/0.4546\n",
      "corr(n0, x0): 0.2080/0.2080\n",
      "corr(n0, x3): 0.0207/0.0207\n",
      "corr(n0, x5): 0.0063/0.0063\n",
      "corr(n0, x7): -0.0553/-0.0553\n",
      "\n",
      "Node 13\n",
      "corr(n13, x0**2): -0.7831/-0.7831\n",
      "corr(n13, cos(x3)): -0.0208/-0.0208\n",
      "corr(n13, x5*x7): -0.5009/-0.5009\n",
      "corr(n13, x0): 0.0615/0.0615\n",
      "corr(n13, x3): -0.0640/-0.0640\n",
      "corr(n13, x5): -0.0137/-0.0137\n",
      "corr(n13, x7): 0.0306/0.0306\n",
      "\n",
      "Node 12\n",
      "corr(n12, x0**2): 0.4115/0.4115\n",
      "corr(n12, cos(x3)): 0.4141/0.4141\n",
      "corr(n12, x5*x7): 0.5182/0.5182\n",
      "corr(n12, x0): -0.0736/-0.0736\n",
      "corr(n12, x3): -0.0292/-0.0292\n",
      "corr(n12, x5): -0.1286/-0.1286\n",
      "corr(n12, x7): 0.1009/0.1009\n"
     ]
    }
   ],
   "source": [
    "ut.node_correlations(acts, nodes, corr_data, nonzero=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "540f5367292c701f747780e6da702d3852f1c7c25c6067d18f36ab6562a2dcf2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
