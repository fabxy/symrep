{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2022 Flatiron Machine Learning x Science Summer School\n",
    "\n",
    "## Step 3: Train plain MLP\n",
    "\n",
    "In this step, we train plain multilayer perceptrons (MLP) to approximate the generic data of the various functions $f \\circ g$ created in Step 1.\n",
    "\n",
    "We set up the training pipeline and explore hyperparameters.\n",
    "\n",
    "The current status of chaos is that training `F01(X01)` and `G01(X01)` did not work well adhoc, thus, we created `F04(X04)`.\n",
    "\n",
    "Furthermore, training on a Google Colab GPU was not successful, so training on CPUs it is.\n",
    "\n",
    "Ideally, we would train either locally with a script or on Colab with a pure training notebook. This notebook should be for analysis only.\n",
    "\n",
    "Also, let's send the results to Weights & Biases.\n",
    "\n",
    "```\n",
    "!pip install wandb --upgrade\n",
    "\n",
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "config = hp\n",
    "\n",
    "with wandb.init(project=\"XY\", config=hyperparameters):\n",
    "    config = wandb.config\n",
    "\n",
    "    ... = make(config)\n",
    "\n",
    "    train(...)\n",
    "\n",
    "    test(...)\n",
    "\n",
    "    return model\n",
    "\n",
    "wandb.watch(model, criterion, log=\"all\", log_freq=10)\n",
    "\n",
    "def train_log(loss, example_ct, epoch):\n",
    "    loss = float(loss)\n",
    "    wandb.log({\"epoch\": epoch, \"loss\": loss}, step=example_ct)\n",
    "\n",
    "def test(...):\n",
    "    with torch.no_grad():\n",
    "        wandb.log({\"test_accuracy\": correct/total})\n",
    "    torch.onnx.export(model, images, \"name\")\n",
    "    wandb.save(\"name\")\n",
    "```\n",
    "\n",
    "Sweeps:\n",
    "```\n",
    "sweep_config = {\n",
    "    \"method\": \"random\"  # or grid or bayesian\n",
    "}\n",
    "\n",
    "metric = {\n",
    "    \"name\": \"loss\",\n",
    "    \"goal\": \"minimize\",\n",
    "}\n",
    "\n",
    "sweep_config[\"metric\"] = metric\n",
    "\n",
    "parameters_dict = {\n",
    "    \n",
    "    \"layer_size\": {\n",
    "        \"values\": [128, 256, 512],\n",
    "    }\n",
    "\n",
    "    \"learning_rate\": {\n",
    "        'distribution': 'uniform',      # q_log_uniform\n",
    "        'min': 0,\n",
    "        'max': 0.1,\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_config[\"parameters\"] = parameters_dict\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"name\")\n",
    "\n",
    "wandb.agent(sweep_id, train, count=5)\n",
    "``` \n",
    "\n",
    "Are the results on `F04(X04)` accurate enough for reconstruction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from srnet import SRNet, SRData, run_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05686bd7f69a461f86ac9d898d053e42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 9.532e+01\n",
      "Total validation loss: 1.012e+02\n"
     ]
    }
   ],
   "source": [
    "# set device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# load data\n",
    "data_path = \"data\"\n",
    "\n",
    "in_var = \"X04\"\n",
    "lat_var = None\n",
    "target_var = \"F04\"\n",
    "\n",
    "mask_ext = \".mask\"\n",
    "masks = joblib.load(os.path.join(data_path, in_var + mask_ext))     # TODO: create mask if file does not exist\n",
    "\n",
    "train_data = SRData(data_path, in_var, lat_var, target_var, masks[\"train\"], device=device)\n",
    "val_data = SRData(data_path, in_var, lat_var, target_var, masks[\"val\"], device=device)\n",
    "\n",
    "# define hyperparameters\n",
    "hyperparams = {\n",
    "    \"arch\": {\n",
    "        \"in_size\": train_data.in_data.shape[1],\n",
    "        \"out_size\": train_data.target_data.shape[1],\n",
    "        \"hid_num\": 3,\n",
    "        \"hid_size\": 25, \n",
    "        \"hid_type\": \"MLP\",\n",
    "        \"lat_size\": 10,\n",
    "        },\n",
    "    \"epochs\": 5000,\n",
    "    \"runtime\": None,\n",
    "    \"batch_size\": 50,\n",
    "    \"lr\": 1e-4,                                                     # TODO: adaptive learning rate?\n",
    "    \"wd\": 1e-4,\n",
    "    # \"l1\": 1e-4,\n",
    "    \"shuffle\": True,\n",
    "}\n",
    "\n",
    "res = run_training(SRNet, hyperparams, train_data, val_data, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots()\n",
    "\n",
    "# plot training loss\n",
    "lines = ax.plot(res['train_loss'])\n",
    "\n",
    "# plot validation loss\n",
    "ax.plot(res['val_loss'], '--', color=lines[-1].get_color())\n",
    "\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this notebook on Google Colab, the following commands are required:\n",
    "\n",
    "`!git clone https://github.com/fabxy/symrep.git`\n",
    "\n",
    "`%cd symrep`\n",
    "\n",
    "However, running the training on a GPU is actually slower than on a CPU. (19.26it/s vs. 31.04it/s)\n",
    "\n",
    "`wandb.watch` slows down the training process ()\n",
    "\n",
    "`num_workers` also slows down the training process (7.38it/s vs. 17.88it/s for `num_workers=2`)\n",
    "\n",
    "torch.backends.cudnn.benchmark (for const batch size)\n",
    "\n",
    "accelerate\n",
    "\n",
    "TensorDataset\n",
    "DataLoader: num_workers, shuffle, batch_size\n",
    "\n",
    "lighting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "540f5367292c701f747780e6da702d3852f1c7c25c6067d18f36ab6562a2dcf2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
