{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f85b9d1",
   "metadata": {},
   "source": [
    "# 2022 Flatiron Machine Learning x Science Summer School\n",
    "\n",
    "## Step 18: Compare training DSN on `F07` with fixed vs. trainable symbolic discriminator\n",
    "\n",
    "First, we train a Disentangled Sparsity Network (DSN) using with a symbolic discriminator (SD), which is pre-trained using the BCE loss to classify function library data vs. untrained MLP activations, for regularization on a problem with a larger function library, namely `F07`.\n",
    "\n",
    "Next, we train the DSN and the SD in parallel in an adversarial setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b38eb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import ipywidgets as ipw\n",
    "import joblib\n",
    "\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "from srnet import SRNet, SRData\n",
    "from sdnet import SDNet, SDData\n",
    "import srnet_utils as ut"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a163e81",
   "metadata": {},
   "source": [
    "### Step 18.1: Train symbolic discriminator with BCE loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "873ef618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fdd75c9c90e4f21b1698098bf2ffce1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ut.plot_disc_accuracies(\"disc_model_F07_v2_fixed_BCE\", \"models\", excl_names=[], avg_hor=50, uncertainty=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a911ec",
   "metadata": {},
   "source": [
    "It's so good, did it work correctly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "505122d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data_path = \"data_1k\"\n",
    "in_var = \"X07\"\n",
    "lat_var = \"G07\"\n",
    "target_var = \"F07\"\n",
    "\n",
    "mask_ext = \".mask\"\n",
    "try:\n",
    "    masks = joblib.load(os.path.join(data_path, in_var + mask_ext))\n",
    "    mask = masks['train']\n",
    "except:\n",
    "    mask = None\n",
    "    print(\"Warning: No mask for training loaded.\")\n",
    "\n",
    "train_data = SRData(data_path, in_var, lat_var, target_var, data_mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32325aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load function library\n",
    "fun_path = \"funs/F07_v2.lib\"\n",
    "shuffle = False\n",
    "iter_sample = False\n",
    "disc_data = SDData(fun_path, in_var, shuffle=shuffle, iter_sample=iter_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7f06c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load trained critic\n",
    "critic = ut.load_disc(\"disc_model_F07_v2_fixed_BCE.pkl\", save_path=\"models\", disc_cls=SDNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f29617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create fake data\n",
    "hp = {\n",
    "    \"arch\": {\n",
    "        \"in_size\": 2,\n",
    "        \"out_size\": 1,\n",
    "        \"hid_num\": (2,0),\n",
    "        \"hid_size\": 32, \n",
    "        \"hid_type\": (\"MLP\", \"MLP\"),\n",
    "        \"hid_kwargs\": {\n",
    "            \"alpha\": None,\n",
    "            \"norm\": None,\n",
    "            \"prune\": None,\n",
    "            },\n",
    "        \"lat_size\": 3,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ffd099",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SRNet(**hp['arch'])\n",
    "with torch.no_grad():\n",
    "    preds, acts = model(train_data.in_data, get_lat=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c69c8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "critic(acts.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e27647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create real data\n",
    "data_real = disc_data.get(disc_data.len, train_data.in_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14da236",
   "metadata": {},
   "outputs": [],
   "source": [
    "critic(data_real[0,:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7d2a47",
   "metadata": {},
   "source": [
    "### Step 18.2: Train DSN with fixed SD\n",
    "\n",
    "Let's use the trained SD to regularize the DSN when training on `F07` data.\n",
    "\n",
    "We also want to track the critic predictions.\n",
    "\n",
    "Currently, we track:\n",
    "\n",
    "* Training loss per epoch and averaged over batches\n",
    "\n",
    "* Validation loss every `log_freq` epochs\n",
    "\n",
    "* `min_corr` on validation data every `log_freq` epochs (a more frequent computation might be expensive)\n",
    "\n",
    "Let's also track the critic prediction per epoch averaged over latent features and batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17d498b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e402ef2cdbd1497b861168d8adad7273",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "models = ut.plot_losses(\"srnet_model_F07_v2_critic_check_lr_1e-4_sd_1e-4\", \"models\", excl_names=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5a737a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4d49c3331844cfd805e5038ef27e8af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "models = ut.plot_corrs(\"srnet_model_F07_v2_critic_check_lr_1e-4_sd_1e-4\", \"models\", excl_names=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f700137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34986680c84141fc9c486c76fc87f8bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "models = ut.plot_disc_preds(\"srnet_model_F07_v2_critic_check_lr_1e-4_sd_1e-4\", \"models\", excl_names=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14dcd9f",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "* Up to 5000 epochs, the training with linear and sigmoid predictions is basically equivalent\n",
    "\n",
    "* At this point, the average critic prediction is already around 1e2 and the sigmoid of the negative of this prediction would be 0\n",
    "\n",
    "* This only makes sense if the critic regularization is not contributing to the total training loss\n",
    "\n",
    "Let's track the critic regularization loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f98abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ut.plot_losses(\"srnet_model_F07_v2_critic_check_lr_1e-4_sd_1e-4\", \"models\", excl_names=[], disc_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2868dfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ut.plot_reg_percentage(\"srnet_model_F07_v2_critic_check_lr_1e-4_sd_1e-4\", \"models\", excl_names=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d5eed7",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "* Using sigmoid predictions (`sig`), the SD is not regularizing the DSN at all\n",
    "\n",
    "* Using linear predictions (`lin`), the prediction and regularization losses have roughly the same magnitude at around 11,000 epochs and afterwards, `min_corr` and `disc_preds` are decreasing and increasing rapidly\n",
    "\n",
    "How much do we need to increase `sd`, so that the sigmoid predictions of the critic are relevant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0645f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ut.plot_losses(\"srnet_model_F07_v2_critic_check_lr_1e-4\", \"models\", excl_names=[\"lin\"], log=True, disc_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77746730",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ut.plot_reg_percentage(\"srnet_model_F07_v2_critic_check_lr_1e-4\", \"models\", excl_names=[\"lin\"], log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed99a485",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ut.plot_disc_preds(\"srnet_model_F07_v2_critic_check_lr_1e-4\", \"models\", excl_names=[\"lin\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76868b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ut.plot_corrs(\"srnet_model_F07_v2_critic_check_lr_1e-4\", \"models\", excl_names=[\"lin\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f3a657",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "* Even for `sd = 1e2`, the regularization loss is only really relevant for the first ten epochs\n",
    "\n",
    "* Afterwards, it's 3 to 4 orders of magnitude smaller, until it drops entirely somewhere between 1000 to 5000 epochs\n",
    "\n",
    "* Interestingly, these small regularization losses seem sufficient to yield different, i.e. worse, `min_corr` values for high `sd` values\n",
    "\n",
    "Anyway, we will need to train DSN and SD in parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddd854b",
   "metadata": {},
   "source": [
    "### Step 18.3: Train DSN and SD in parallel\n",
    "\n",
    "Let's first explore training the DSN and SD in parallel by comparing different approaches to utilize the critic predictions for regularization.\n",
    "\n",
    "Note that the DSN wants to maximize critic predictions, i.e. learning latent features that are similar to data from the function library.\n",
    "\n",
    "1. `linear`: Simply take the negative value of the critic predictions, so minimizing the total loss is maximizing critic predictions\n",
    "\n",
    "2. `sigmoid`: Take the sigmoid of the negative critic predictions, so positive values yield regularization losses between 0 and 0.5 and negative losses between 0.5 and 1\n",
    "\n",
    "3. `logsigmoid`: Take the negative of the logarithm of the sigmoid of the critic predictions, so positive values yield sigmoid values between 0.5 and 1, the logarithm of which yields small negative numbers. Negative values yield sigmoid values between 0 and 0.5, the logarithm of which yields large negative numbers (nearly linear with the original critic predictions)\n",
    "\n",
    "The generator in standard GAN training minimizes `logsigmoid` (https://youtu.be/OljTVUVzPpM): \n",
    "\n",
    "* `min log(1 - sig(D(fake)))` -> `max log(sig(D(fake)))` due to saturating gradients, however:\n",
    "\n",
    "    * `min log(1 - D(fake))` is equal to `max fake + log(1 + exp(-fake))`\n",
    "\n",
    "    * `max log(sig(D(fake)))` is equal to `min log(1 + exp(-fake))`\n",
    "\n",
    "Also standard GAN training:\n",
    "\n",
    "* Uses `LeakyReLU` for generator and discriminator\n",
    "\n",
    "* Uses the same learning rate for generator and discriminator (try magic `3e-4`)\n",
    "\n",
    "* Uses `BCELoss` for the discriminator (as we do now): `max log(sig(D(real))) + log(1 - sig(D(fake)))`\n",
    "\n",
    "* Runs only one iteration of discriminator training per epoch\n",
    "\n",
    "**Q**: One difference of our training approach is that the fake data is basically fixed, i.e. it changes only slightly per epoch and cannot be sampled. Is that a problem? Could we do something about it, e.g. dropout?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53c77542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaa2c0b68e1c468a9e87d6adcf398e09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "models = ut.plot_losses(\"srnet_model_F07_v2_comb_check\", \"models\", excl_names=[\"arch\"], disc_loss=False, log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c2d96a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f706ef2a2e7947f29a522f1d11efae44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "models = ut.plot_reg_percentage(\"srnet_model_F07_v2_comb_check\", \"models\", excl_names=[\"arch\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e10e8331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fa1da1f996947a29d27600ab51bd97c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "models = ut.plot_corrs(\"srnet_model_F07_v2_comb_check\", \"models\", excl_names=[\"arch\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a90bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ut.plot_disc_preds(\"srnet_model_F07_v2_comb_check\", \"models\", excl_names=[\"arch\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e7e877",
   "metadata": {},
   "source": [
    "* Using sigmoid predictions (`sig`):\n",
    "\n",
    "    * The regularization loss dominates the total loss after 12000 epochs.\n",
    "    \n",
    "    * However, the prediction loss keeps decreasing, while the regularization loss is constant unity and `min_corr` decreases.\n",
    "    \n",
    "    * The average SD predictions are very low. Does the critic not learn anything?\n",
    "\n",
    "* Using linear predictions (`lin`):\n",
    "\n",
    "    * The results are very promising.\n",
    "    \n",
    "    * While the training is quite unstable (what is normal for GAN training?), `min_corr` reaches near 100% correlation at an acceptable validation loss. \n",
    "    \n",
    "    * Compared to the `sig` results, the average SD predictions are moderate and the regularization loss never completely dominates the total loss.\n",
    "\n",
    "* Using `logsigmoid` predictions (`log`):\n",
    "\n",
    "    * Similar to `lin` for large negative critic predictions, but converges to 0 instead of negative values for large positive critic predictions. \n",
    "    \n",
    "    * While the average SD predictions and the regularization to prediction loss ratio are roughly similar and slightly lower validation errors are achieved, a similar level of `min_corr` is not reached.\n",
    "\n",
    "Note that all of these observations are specific to the one set of selected hyperparameters.\n",
    "\n",
    "Let's inspect the recorded latent activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffa75d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data_path = \"data_1k\"\n",
    "in_var = \"X07\"\n",
    "lat_var = \"G07\"\n",
    "target_var = \"F07\"\n",
    "\n",
    "mask_ext = \".mask\"\n",
    "try:\n",
    "    masks = joblib.load(os.path.join(data_path, in_var + mask_ext))\n",
    "    mask = masks['val']\n",
    "except:\n",
    "    mask = None\n",
    "    print(\"Warning: No mask for training loaded.\")\n",
    "\n",
    "val_data = SRData(data_path, in_var, lat_var, target_var, data_mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94cdf13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = joblib.load(\"models/srnet_model_F07_v2_comb_check_lr_1e-4_sd_1e0_lin.pkl\")\n",
    "rec_acts = joblib.load(\"models/srnet_model_F07_v2_comb_check_lr_1e-4_sd_1e0_lin.rec\")\n",
    "\n",
    "epochs = len(state['train_loss'])\n",
    "logs = len(rec_acts)\n",
    "\n",
    "add_log = epochs % (logs - 1) == 0\n",
    "log_freq = int(epochs / (logs-add_log))\n",
    "\n",
    "log_epochs = (np.arange(logs) * log_freq).tolist()\n",
    "log_epochs[-1] = epochs - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10f1ad15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ba68d01fda247d3ae47a8467925522d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ba52bb4bb624e10855de8dbc8f3e707",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(SelectionSlider(continuous_update=False, description='Epoch', options=(0, 25, 50, 75, 10…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "node_inputs = {\n",
    "    0: [0],\n",
    "    1: [1],\n",
    "    2: [0, 1],\n",
    "}\n",
    "\n",
    "fig_width = 9\n",
    "view = (6, -92)\n",
    "\n",
    "w_epoch = ipw.SelectionSlider(\n",
    "    options=log_epochs,\n",
    "    value=0,\n",
    "    description=\"Epoch\",\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    "    readout=True\n",
    ")\n",
    "\n",
    "fig_num = len(node_inputs)\n",
    "fig_ar = plt.rcParams['figure.figsize'][0] / plt.rcParams['figure.figsize'][1]\n",
    "fig = plt.figure(figsize=(fig_width, fig_width/fig_ar/fig_num))\n",
    "\n",
    "for n in node_inputs:\n",
    "    if len(node_inputs[n]) == 1:\n",
    "        ax = fig.add_subplot(1, fig_num, n+1)\n",
    "    else:\n",
    "        ax = fig.add_subplot(1, fig_num, n+1, projection='3d')\n",
    "        ax.view_init(elev=view[0], azim=view[1])\n",
    "\n",
    "def update_plot(epoch):\n",
    "    \n",
    "    e = log_epochs.index(epoch)\n",
    "       \n",
    "    for n in node_inputs:\n",
    "        \n",
    "        ax = fig.axes[n]\n",
    "        ax.clear()\n",
    "        \n",
    "        if len(node_inputs[n]) == 1:\n",
    "            \n",
    "            i = node_inputs[n][0]\n",
    "            \n",
    "            ax.scatter(val_data.in_data[:,i], val_data.lat_data[:,n])\n",
    "            ax.scatter(val_data.in_data[:,i], rec_acts[e][:,n])\n",
    "            \n",
    "        else:\n",
    "                                    \n",
    "            i = node_inputs[n][0]\n",
    "            j = node_inputs[n][1]\n",
    "                    \n",
    "            ax.scatter3D(val_data.in_data[:,i], val_data.in_data[:,j], val_data.lat_data[:,n])\n",
    "            ax.scatter3D(val_data.in_data[:,i], val_data.in_data[:,j], rec_acts[e][:,n])\n",
    "\n",
    "ipw.interact(update_plot, epoch=w_epoch);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1330cf47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef2a015f17434b11a651f41837f67437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "models = ut.plot_corrs(\"srnet_model_F07_v2_comb_check_lr_1e-4_sd_1e0_lin.pkl\", \"models\", excl_names=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adfccef",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "* Between epoch 20000 and 40000, the quadratic function (node 0) and the $x_0 \\cdot x_1$ function (node 2) are approximated very accurately and the training effort seems to go into modeling the cosine function (node 1)\n",
    "\n",
    "* Is the capacity of the DSN too low to accurately capture the cosine function?\n",
    "\n",
    "Let's increase the DSN architecture from two hidden layers with 32 nodes (`arch_S`) to three hidden layers with 64 nodes (`arch_M`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bca787b",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ut.plot_losses(\"srnet_model_F07_v2_comb_check_lr_1e-4_sd_1e0_lin\", \"models\", excl_names=[], disc_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27b87cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ut.plot_reg_percentage(\"srnet_model_F07_v2_comb_check_lr_1e-4_sd_1e0_lin\", \"models\", excl_names=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e6efb28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d6af6fb03594989908994301d4f9f7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "models = ut.plot_corrs(\"srnet_model_F07_v2_comb_check_lr_1e-4_sd_1e0_lin\", \"models\", excl_names=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ab431a",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ut.plot_disc_preds(\"srnet_model_F07_v2_comb_check_lr_1e-4_sd_1e0_lin\", \"models\", excl_names=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383eee42",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "* These results are great, `min_corr` values of nearly 100% are achieved!\n",
    "\n",
    "* However, there is instability in the training process. Can this be improved by adapting the hyperparameters?\n",
    "\n",
    "* Also, can similar results be achieved with `sig` or `log` predictions?\n",
    "\n",
    "Let's run a hyperparameter study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18a7751",
   "metadata": {},
   "source": [
    "### Step 18.4: Run hyperparameter study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d48a24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set wandb project\n",
    "wandb_project = \"184-DSN-SD-comb-study-F07_v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3a8243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams = {\n",
    "#     \"arch\": {\n",
    "#         \"in_size\": train_data.in_data.shape[1],\n",
    "#         \"out_size\": train_data.target_data.shape[1],\n",
    "#         \"hid_num\": (2, 0),\n",
    "#         \"hid_size\": 32,\n",
    "#         \"hid_type\": (\"DSN\", \"MLP\"),\n",
    "#         \"hid_kwargs\": {\n",
    "#             \"alpha\": [[1,0],[0,1],[1,1]],\n",
    "#             \"norm\": None,\n",
    "#             \"prune\": None,\n",
    "#             },\n",
    "#         \"lat_size\": 3,\n",
    "#         },\n",
    "#     \"epochs\": 50000,\n",
    "#     \"runtime\": None,\n",
    "#     \"batch_size\": train_data.in_data.shape[0],\n",
    "#     \"shuffle\": False,\n",
    "#     \"lr\": 1e-4,\n",
    "#     \"wd\": 1e-7,\n",
    "#     \"l1\": 0.0,\n",
    "#     \"a1\": 0.0,\n",
    "#     \"a2\": 0.0,\n",
    "#     \"e1\": 0.0,\n",
    "#     \"e2\": 0.0,\n",
    "#     \"e3\": 0.0,\n",
    "#     \"gc\": 0.0,\n",
    "#     \"sd\": 1.0,\n",
    "#     \"sd_fun\": \"linear\",\n",
    "#     \"ext\": None,\n",
    "#     \"ext_type\": None,\n",
    "#     \"ext_size\": 0,\n",
    "#     \"disc\": {\n",
    "#         \"hid_num\": 2,\n",
    "#         \"hid_size\": 64,\n",
    "#         \"lr\": 1e-4,\n",
    "#         \"wd\": 1e-7,\n",
    "#         \"betas\": (0.9,0.999),\n",
    "#         \"iters\": 5,\n",
    "#         \"gp\": 0.0,\n",
    "#         \"loss_fun\": \"BCE\",\n",
    "#     },\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635b6c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameter study\n",
    "hp_study = {\n",
    "    \"method\": \"random\",\n",
    "    \"parameters\": {\n",
    "        \"arch\": {\n",
    "            \"parameters\": {\n",
    "                \"in_size\": {\n",
    "                    \"values\": [2]\n",
    "                },\n",
    "                \"out_size\": {\n",
    "                    \"values\": [1]\n",
    "                },\n",
    "                \"hid_num\": {\n",
    "                    \"values\": [(2,0), (3,0), (4,0)]\n",
    "                },\n",
    "                \"hid_size\": {\n",
    "                    \"values\": [32, 64, 128]\n",
    "                },\n",
    "                \"hid_type\": {\n",
    "                    \"values\": [(\"DSN\", \"MLP\")]\n",
    "                },\n",
    "                \"hid_kwargs\": {\n",
    "                    \"values\": [{\"alpha\": [[1,0],[0,1],[1,1]]}]\n",
    "                },\n",
    "                \"lat_size\": {\n",
    "                    \"values\": [3]\n",
    "                },\n",
    "            }\n",
    "        },\n",
    "        \"lr\": {\n",
    "            \"values\": [1e-5, 1e-4, 1e-3]\n",
    "        },\n",
    "        \"sd\": {\n",
    "            \"values\": [1e-4, 1e-2, 1e0, 1e2]\n",
    "        },\n",
    "        \"sd_fun\": {\n",
    "            \"values\": [\"linear\", \"sigmoid\", \"logsigmoid\"]\n",
    "        },\n",
    "        \"disc\": {\n",
    "            \"parameters\": {\n",
    "                \"hid_num\": {\n",
    "                    \"values\": [2]\n",
    "                },\n",
    "                \"hid_size\": {\n",
    "                    \"values\": [64]\n",
    "                },\n",
    "                \"lr\": {\n",
    "                    \"values\": [1e-5, 1e-4, 1e-3]\n",
    "                },\n",
    "                \"iters\": {\n",
    "                    \"values\": [1, 5, 10]\n",
    "                },\n",
    "                \"loss_fun\": {\n",
    "                    \"values\": [\"BCE\"]\n",
    "                },\n",
    "            }\n",
    "        },\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd56d95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sweep\n",
    "sweep_id = wandb.sweep(hp_study, project=wandb_project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "82461ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"results/184-DSN-SD-comb-study-F07_v2.csv\")\n",
    "\n",
    "model_base_name = \"srnet_model_F07_v2_DSN_SD_comb_study\"\n",
    "df[\"Name\"] = df[\"Name\"].str.replace(model_base_name + \"_\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f14af85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.style.format({'sd': '{:.0e}'.format,'lr': '{:.0e}'.format,'disc.lr': '{:.0e}'.format})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9b3ee9",
   "metadata": {},
   "source": [
    "Top results:\n",
    "\n",
    "* `sigmoid`: \n",
    "\n",
    "    * Steady and `min_corr` value of 0.85\n",
    "\n",
    "* `linear`: \n",
    "\n",
    "    * Steady and `min_corr` value of 0.99\n",
    "    \n",
    "    * 17/34 runs achieve `min_corr > 0.95` during training:\n",
    "    \n",
    "        * Stable (3x): `sd = 1e-4`, `lr <= disc.lr`, `disc.lr = 1e-3`\n",
    "        \n",
    "        * Late (3x): `sd = 1e-2`, `lr = 1e-5`\n",
    "        \n",
    "        * Unstable: `sd = 1e-2` or `sd = 1e0`\n",
    "\n",
    "* `logsigmoid`: \n",
    "\n",
    "    * Instable or late convergence to `min_corr` value of 0.99\n",
    "    \n",
    "    * Only 5/34 runs achieve `min_corr > 0.95` during training:\n",
    "    \n",
    "        * Late (2x): `sd = 1e-2`, `lr <= disc.lr`, `disc.lr = 1e-3`\n",
    "        \n",
    "        * Semi-stable (1x): `sd = 1e-2`, `lr = 1e-5`\n",
    "        \n",
    "        * Unstable (2x): `sd = 1e0`, `lr > disc.lr`\n",
    "        \n",
    "Notes:\n",
    "\n",
    "* `hid_num >= 3` required\n",
    "\n",
    "* `hid_size` and `iters` inconclusive\n",
    "\n",
    "Let's plot the best model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5839a949",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"76k8u8dh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9d4cbac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ce4b9a6ad984ae8847f65d8e18075b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "models = ut.plot_losses(model_id, save_path=\"models\", excl_names=[], disc_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1455f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ut.plot_reg_percentage(model_id, \"models\", excl_names=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7c2c0db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "936d641130c241b2b9acfa3ad7df613b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "models = ut.plot_corrs(model_id, \"models\", excl_names=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706abddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ut.plot_disc_preds(model_id, \"models\", excl_names=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1230e6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = '_'.join([model_base_name, model_id])\n",
    "model_path = \"models\"\n",
    "model_ext = \".pkl\"\n",
    "\n",
    "model = ut.load_model(model_name + model_ext, model_path, SRNet)\n",
    "\n",
    "with torch.no_grad():\n",
    "    preds, acts = model(train_data.in_data, get_lat=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed7a7949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d95ebef8d7b4863850bedf3e8f183a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "n = 0\n",
    "bias = True\n",
    "\n",
    "ax.scatter(train_data.in_data[:,n], train_data.lat_data[:,n])\n",
    "ax.scatter(train_data.in_data[:,n], model.layers2[0].weight[0,n].item()*acts[:,n] + bias * model.layers2[0].bias.item())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c20c2e7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed09206056574905af4528d6facc6408",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "n = 1\n",
    "bias = True\n",
    "\n",
    "ax.scatter(train_data.in_data[:,n], train_data.lat_data[:,n])\n",
    "ax.scatter(train_data.in_data[:,n], model.layers2[0].weight[0,n].item()*acts[:,n] + bias * model.layers2[0].bias.item())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9279acc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2850ce1698dd49afb65a1f614760f1a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 2\n",
    "z_data = [(\"x*y\", train_data.lat_data[:,n])]\n",
    "plot_size = train_data.target_data.shape[0]\n",
    "\n",
    "ut.plot_acts(train_data.in_data[:,0], train_data.in_data[:,1], z_data, acts=acts, nodes=[n], model=model, bias=False, nonzero=False, agg=False, plot_size=plot_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30df32bc",
   "metadata": {},
   "source": [
    "These results are great!\n",
    "\n",
    "What are reasonable next steps?\n",
    "\n",
    "Let's check the current assumptions and limitations:\n",
    "\n",
    "* Latent size is known (bottleneck DSN)\n",
    "\n",
    "* Input dependency is known (entropy regularization)\n",
    "\n",
    "* Definition of function library (functions, coefficients)\n",
    "\n",
    "* Linear function $f(x)$\n",
    "\n",
    "* No noise\n",
    "\n",
    "I do think that we should keep it a three step framework for now.\n",
    "\n",
    "More interesting next steps could be:\n",
    "\n",
    "* Removing coefficients in the function library\n",
    "\n",
    "* Extending function library\n",
    "\n",
    "* Building hierarchical models\n",
    "\n",
    "Technical ideas:\n",
    "\n",
    "* Include gradients\n",
    "\n",
    "* Add linear transformations to input and output of cell\n",
    "\n",
    "* Gradient clipping"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
