{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f89b55e7",
   "metadata": {},
   "source": [
    "# 2022 Flatiron Machine Learning x Science Summer School\n",
    "\n",
    "## Step 7: Explore regularization for bottleneck DSN\n",
    "\n",
    "What we have seen so far:\n",
    "\n",
    "* For problem `F06` without degeneracies, the latent features can be discovered perfectly using the DSN (no GhostAdam required)\n",
    "\n",
    "* For problem `F00` with degeneracies, the latent features cannot be discovered, even when:\n",
    "\n",
    "    * Using the DSN with GhostAdam\n",
    "    \n",
    "    * Training for 100k epochs\n",
    "    \n",
    "    * Increasing $a_2$\n",
    "\n",
    "It seems that depending on as few input features as possible is not enforced enough or there is a different (undesired) global minimum, e.g. collapsing into a single latent feature (which happens when training for 100k epochs). In other words, it seems that enforcing sparsity (`few_latents`) is too similar to enforcing `few_dependencies`.\n",
    "\n",
    "Let's disentangle the problem by creating a bottleneck, i.e. setting the number of latent features to the correct size a-priori.\n",
    "\n",
    "Then, we can compare the following approaches:\n",
    "\n",
    "1. Training DSN\n",
    "\n",
    "2. Training DSN with $a_2 \\ne 0$\n",
    "\n",
    "3. Training DSN with normalized $\\alpha$ and $a_2 \\ne 0$\n",
    "\n",
    "4. Training DSN with normalized $\\alpha$ and minimizing entropy\n",
    "\n",
    "5. Training DSN with normalized $\\alpha$ and minimizing entropy $\\times$ variance\n",
    "\n",
    "Let's go!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb977e3d",
   "metadata": {},
   "source": [
    "### Step 7.1: Train bottleneck DSN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f72d1241",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "from srnet import SRNet, SRData\n",
    "import srnet_utils as ut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58e0538a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set wandb project\n",
    "wandb_project = \"71-bn-DSN-F00\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1595d1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams = {\n",
    "#     \"arch\": {\n",
    "#         \"in_size\": train_data.in_data.shape[1],\n",
    "#         \"out_size\": train_data.target_data.shape[1],\n",
    "#         \"hid_num\": (2,0),\n",
    "#         \"hid_size\": 32, \n",
    "#         \"hid_type\": (\"DSN\", \"MLP\"),\n",
    "#         \"lat_size\": 3,\n",
    "#         },\n",
    "#     \"epochs\": 10000,\n",
    "#     \"runtime\": None,\n",
    "#     \"batch_size\": 64,\n",
    "#     \"lr\": 1e-4,\n",
    "#     \"wd\": 1e-4,\n",
    "#     \"l1\": 0.0,\n",
    "#     \"a1\": 0.0,\n",
    "#     \"a2\": 0.0,\n",
    "#     \"gc\": 0.0,\n",
    "#     \"shuffle\": True,\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b74b87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data from wandb\n",
    "file_ext = \".pkl\"\n",
    "\n",
    "api = wandb.Api()\n",
    "\n",
    "runs = api.runs(wandb_project)\n",
    "for run in runs:\n",
    "    for f in run.files():\n",
    "        if f.name[-len(file_ext):] == file_ext and not os.path.isfile(f.name):\n",
    "            print(f\"Downloading {os.path.basename(f.name)}.\")\n",
    "            run.file(f.name).download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "422bfc70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac21ead2fae84142a5079f637e0adb95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot losses\n",
    "save_names = [\"srnet_model_F00_bn\"]\n",
    "save_path = \"models\"\n",
    "models = ut.plot_losses(save_names, save_path=\"models\", excl_names=[\"norm\", \"a2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f5b78ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data_path = \"data_1k\"\n",
    "\n",
    "in_var = \"X00\"\n",
    "lat_var = \"G00\"\n",
    "target_var = \"F00\"\n",
    "\n",
    "mask_ext = \".mask\"\n",
    "masks = joblib.load(os.path.join(data_path, in_var + mask_ext))     # TODO: create mask if file does not exist\n",
    "\n",
    "train_data = SRData(data_path, in_var, lat_var, target_var, masks[\"train\"])\n",
    "val_data = SRData(data_path, in_var, lat_var, target_var, masks[\"val\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd793504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "srnet_model_F00_bn\n",
      "[5.482872, 0.33377925, 1.2634401e-06]\n",
      "[1, 0, 2]\n",
      "[[-2.1264930e+00  9.6783280e-01]\n",
      " [ 1.5003376e+00 -6.6390908e-01]\n",
      " [-3.6289787e-01 -7.9980089e-10]]\n",
      "[[-2.1264930e+00  9.6783280e-01]\n",
      " [ 1.5003376e+00 -6.6390908e-01]\n",
      " [-3.6289787e-01 -7.9980089e-10]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# overview latent feature variance and alpha matrix\n",
    "model_path = \"models\"\n",
    "model_ext = \".pkl\"\n",
    "\n",
    "alpha_eps = 1e-6\n",
    "alpha_bin = False\n",
    "\n",
    "for model_name in models:\n",
    "    print(model_name)\n",
    "    model = ut.load_model(model_name + model_ext, model_path, SRNet)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        preds, acts = model(train_data.in_data, get_lat=True)\n",
    "        \n",
    "    all_nodes = ut.get_node_order(acts, show=True)\n",
    "    \n",
    "    if alpha_eps:\n",
    "        alpha = model.layers1.alpha.detach().cpu().numpy()[all_nodes]\n",
    "        \n",
    "        if alpha_bin:\n",
    "            alpha[np.abs(alpha) < alpha_eps] = 0\n",
    "            alpha[np.abs(alpha) > alpha_eps] = 1\n",
    "        \n",
    "        print(alpha[np.abs(alpha).sum(axis=1) > alpha_eps])\n",
    "        \n",
    "    print(model.layers1.norm(model.layers1.alpha.detach().cpu()).numpy()[all_nodes])\n",
    "    \n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "755a260a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.482872, 0.33377925, 1.2634401e-06]\n",
      "[1, 0, 2]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"srnet_model_F00_bn\"\n",
    "\n",
    "model = ut.load_model(model_name + model_ext, model_path, SRNet)\n",
    "\n",
    "with torch.no_grad():\n",
    "    preds, acts = model(train_data.in_data, get_lat=True)\n",
    "    \n",
    "all_nodes = ut.get_node_order(acts, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b47e6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = all_nodes[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8941821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select plotting data\n",
    "x_data = train_data.in_data[:,0]\n",
    "y_data = train_data.in_data[:,1]\n",
    "z_data = [\n",
    "    (\"target\", train_data.target_data),\n",
    "    #(\"x**2\", x_data**2), \n",
    "    #(\"cos(y)\", np.cos(y_data)), \n",
    "    #(\"x*y\", x_data * y_data),\n",
    "]\n",
    "plot_size = train_data.target_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24d2295a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b098238a0a248cb80a50acfa597e126",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ut.plot_acts(x_data, y_data, z_data, acts=acts, nodes=nodes, model=model, bias=True, nonzero=False, agg=False, plot_size=plot_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96e14e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_data = [\n",
    "    (\"x**2\", x_data**2), \n",
    "    (\"cos(y)\", np.cos(y_data)), \n",
    "    (\"x*y\", x_data * y_data),\n",
    "    (\"x\", x_data),\n",
    "    (\"y\", y_data),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43a31668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Node 1\n",
      "corr(n1, x**2): -0.8694\n",
      "corr(n1, cos(y)): -0.0501\n",
      "corr(n1, x*y): -0.5165\n",
      "corr(n1, x): -0.1020\n",
      "corr(n1, y): -0.0796\n",
      "\n",
      "Node 0\n",
      "corr(n0, x**2): -0.1374\n",
      "corr(n0, cos(y)): -0.3930\n",
      "corr(n0, x*y): -0.4840\n",
      "corr(n0, x): -0.4720\n",
      "corr(n0, y): 0.0973\n",
      "\n",
      "Node 2\n",
      "corr(n2, x**2): -0.2492\n",
      "corr(n2, cos(y)): -0.0037\n",
      "corr(n2, x*y): -0.1246\n",
      "corr(n2, x): -0.9958\n",
      "corr(n2, y): -0.0252\n"
     ]
    }
   ],
   "source": [
    "ut.node_correlations(acts, nodes, corr_data);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9f1b1c",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "* Latent feature shapes are quite different from previously seen\n",
    "\n",
    "* One $\\alpha$ value goes to zero automatically\n",
    "\n",
    "* Still no dependence (of the high-variance latent features) on individual input features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15067fbf",
   "metadata": {},
   "source": [
    "### Step 7.2: Train bottleneck DSN with $a_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b11ec7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set wandb project\n",
    "wandb_project = \"72-bn-DSN-a2-study-F00\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bed94b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams = {\n",
    "#     \"arch\": {\n",
    "#         \"in_size\": train_data.in_data.shape[1],\n",
    "#         \"out_size\": train_data.target_data.shape[1],\n",
    "#         \"hid_num\": (2,0),\n",
    "#         \"hid_size\": 32, \n",
    "#         \"hid_type\": (\"DSN\", \"MLP\"),\n",
    "#         \"lat_size\": 3,\n",
    "#         },\n",
    "#     \"epochs\": 10000,\n",
    "#     \"runtime\": None,\n",
    "#     \"batch_size\": 64,\n",
    "#     \"lr\": 1e-4,\n",
    "#     \"wd\": 1e-4,\n",
    "#     \"l1\": 0.0,\n",
    "#     \"a1\": 0.0,\n",
    "#     \"a2\": 0.0,\n",
    "#     \"gc\": 0.0,\n",
    "#     \"shuffle\": True,\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "117a8518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameter study\n",
    "hp_study = {\n",
    "    \"method\": \"grid\", # random, bayesian\n",
    "    #\"metric\": {\n",
    "    #    \"name\": \"val_loss\",\n",
    "    #    \"goal\": \"minimize\",\n",
    "    #},\n",
    "    \"parameters\": {\n",
    "        #\"l1\": {\n",
    "        #    \"values\": [1e-4, 1e-3, 1e-2]\n",
    "        #},\n",
    "        \"a2\": {\n",
    "            \"values\": [1e-3, 1e-2, 1e-1, 1e0]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f73ef9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sweep\n",
    "sweep_id = wandb.sweep(hp_study, project=wandb_project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dfd0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data from wandb\n",
    "file_ext = \".pkl\"\n",
    "\n",
    "api = wandb.Api()\n",
    "\n",
    "runs = api.runs(wandb_project)\n",
    "for run in runs:\n",
    "    for f in run.files():\n",
    "        if f.name[-len(file_ext):] == file_ext and not os.path.isfile(f.name):\n",
    "            print(f\"Downloading {os.path.basename(f.name)}.\")\n",
    "            run.file(f.name).download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04525ef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b75a3d545c3343bfa6f7ee82a2232b63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot losses\n",
    "save_names = [\"F00_bn_a2\"]\n",
    "save_path = \"models\"\n",
    "models = ut.plot_losses(save_names, save_path=\"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0608af5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data_path = \"data_1k\"\n",
    "\n",
    "in_var = \"X00\"\n",
    "lat_var = \"G00\"\n",
    "target_var = \"F00\"\n",
    "\n",
    "mask_ext = \".mask\"\n",
    "masks = joblib.load(os.path.join(data_path, in_var + mask_ext))     # TODO: create mask if file does not exist\n",
    "\n",
    "train_data = SRData(data_path, in_var, lat_var, target_var, masks[\"train\"])\n",
    "val_data = SRData(data_path, in_var, lat_var, target_var, masks[\"val\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da48ac3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "srnet_model_F00_bn_a2_1e+00\n",
      "[0.39600012, 8.881784e-16, 1.3877788e-17]\n",
      "[2, 0, 1]\n",
      "[[-0.00895066 -0.00502903]]\n",
      "[[-8.9506609e-03 -5.0290320e-03]\n",
      " [-1.2541069e-18  3.3487419e-18]\n",
      " [ 5.4481973e-07  7.0096348e-08]]\n",
      "\n",
      "srnet_model_F00_bn_a2_1e-01\n",
      "[0.6369252, 0.0004733717, 3.5527137e-15]\n",
      "[2, 0, 1]\n",
      "[[-4.1295648e-02 -2.2897277e-02]\n",
      " [ 1.0456526e-02  8.0807167e-06]]\n",
      "[[-4.1295648e-02 -2.2897277e-02]\n",
      " [ 1.0456526e-02  8.0807167e-06]\n",
      " [ 2.4354176e-09 -1.9725133e-09]]\n",
      "\n",
      "srnet_model_F00_bn_a2_1e-02\n",
      "[0.6763779, 0.17751801, 5.551115e-17]\n",
      "[1, 0, 2]\n",
      "[[-0.20615605  0.08827081]\n",
      " [ 0.19110097 -0.09128037]]\n",
      "[[-2.0615605e-01  8.8270806e-02]\n",
      " [ 1.9110097e-01 -9.1280371e-02]\n",
      " [ 1.3835078e-10  6.2785566e-10]]\n",
      "\n",
      "srnet_model_F00_bn_a2_1e-03\n",
      "[3.9267466, 0.24354573, 2.220446e-16]\n",
      "[1, 0, 2]\n",
      "[[-1.5844834   0.46944278]\n",
      " [ 1.0996279  -0.41996077]]\n",
      "[[-1.5844834e+00  4.6944278e-01]\n",
      " [ 1.0996279e+00 -4.1996077e-01]\n",
      " [-2.6066029e-39 -9.1203791e-40]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# overview latent feature variance and alpha matrix\n",
    "model_path = \"models\"\n",
    "model_ext = \".pkl\"\n",
    "\n",
    "alpha_eps = 1e-2\n",
    "alpha_bin = False\n",
    "\n",
    "for model_name in models:\n",
    "    print(model_name)\n",
    "    model = ut.load_model(model_name + model_ext, model_path, SRNet)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        preds, acts = model(train_data.in_data, get_lat=True)\n",
    "        \n",
    "    all_nodes = ut.get_node_order(acts, show=True)\n",
    "    \n",
    "    if alpha_eps:\n",
    "        alpha = model.layers1.alpha.detach().cpu().numpy()[all_nodes]\n",
    "        \n",
    "        if alpha_bin:\n",
    "            alpha[np.abs(alpha) < alpha_eps] = 0\n",
    "            alpha[np.abs(alpha) > alpha_eps] = 1\n",
    "        \n",
    "        print(alpha[np.abs(alpha).sum(axis=1) > alpha_eps])\n",
    "        \n",
    "    print(model.layers1.norm(model.layers1.alpha.detach().cpu()).numpy()[all_nodes])\n",
    "    \n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883e3e7f",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "* Using $a_2$ to enforce `few_dependencies` has a larger effect on enforcing `few_latents` than on `few_dependencies`\n",
    "\n",
    "* For $a_2$ being `1e-02`, the resulting model has only two high-variance features, but these still each depend on both input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5fc7e383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6763779, 0.17751801, 5.551115e-17]\n",
      "[1, 0, 2]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"srnet_model_F00_bn_a2_1e-02\"\n",
    "\n",
    "model = ut.load_model(model_name + model_ext, model_path, SRNet)\n",
    "\n",
    "with torch.no_grad():\n",
    "    preds, acts = model(train_data.in_data, get_lat=True)\n",
    "    \n",
    "all_nodes = ut.get_node_order(acts, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79c0eaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = all_nodes[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "96b6dbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select plotting data\n",
    "x_data = train_data.in_data[:,0]\n",
    "y_data = train_data.in_data[:,1]\n",
    "z_data = [\n",
    "    (\"target\", train_data.target_data),\n",
    "    #(\"x**2\", x_data**2), \n",
    "    #(\"cos(y)\", np.cos(y_data)), \n",
    "    #(\"x*y\", x_data * y_data),\n",
    "]\n",
    "plot_size = train_data.target_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c4f97000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faa285a35b034292b5eb3eb3b5e3de3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ut.plot_acts(x_data, y_data, z_data, acts=acts, nodes=nodes, model=model, bias=True, nonzero=False, agg=False, plot_size=plot_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f969b196",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_data = [\n",
    "    (\"x**2\", x_data**2), \n",
    "    (\"cos(y)\", np.cos(y_data)), \n",
    "    (\"x*y\", x_data * y_data),\n",
    "    (\"x\", x_data),\n",
    "    (\"y\", y_data),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "42b5ca3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Node 1\n",
      "corr(n1, x**2): -0.8579\n",
      "corr(n1, cos(y)): -0.1630\n",
      "corr(n1, x*y): -0.5906\n",
      "corr(n1, x): -0.2864\n",
      "corr(n1, y): -0.0202\n",
      "\n",
      "Node 0\n",
      "corr(n0, x**2): -0.7611\n",
      "corr(n0, cos(y)): -0.2188\n",
      "corr(n0, x*y): -0.6844\n",
      "corr(n0, x): -0.2229\n",
      "corr(n0, y): -0.0552\n"
     ]
    }
   ],
   "source": [
    "ut.node_correlations(acts, nodes, corr_data);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f937406",
   "metadata": {},
   "source": [
    "### Step 7.3: Train bottleneck DSN with $\\hat{\\alpha}$ and $a_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f5813169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set wandb project\n",
    "wandb_project = \"73-bn-DSN-norm-a2-study-F00\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1b56110d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams = {\n",
    "#     \"arch\": {\n",
    "#         \"in_size\": train_data.in_data.shape[1],\n",
    "#         \"out_size\": train_data.target_data.shape[1],\n",
    "#         \"hid_num\": (2,0),\n",
    "#         \"hid_size\": 32, \n",
    "#         \"hid_type\": (\"DSN\", \"MLP\"),\n",
    "#         \"hid_kwargs\": ({\"norm\": \"softmax\"}, {}),\n",
    "#         \"lat_size\": 3,\n",
    "#         },\n",
    "#     \"epochs\": 10000,\n",
    "#     \"runtime\": None,\n",
    "#     \"batch_size\": 64,\n",
    "#     \"lr\": 1e-4,\n",
    "#     \"wd\": 1e-4,\n",
    "#     \"l1\": 0.0,\n",
    "#     \"a1\": 0.0,\n",
    "#     \"a2\": 0.0,\n",
    "#     \"gc\": 0.0,\n",
    "#     \"shuffle\": True,\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7eb90f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameter study\n",
    "hp_study = {\n",
    "    \"method\": \"grid\", # random, bayesian\n",
    "    #\"metric\": {\n",
    "    #    \"name\": \"val_loss\",\n",
    "    #    \"goal\": \"minimize\",\n",
    "    #},\n",
    "    \"parameters\": {\n",
    "        #\"l1\": {\n",
    "        #    \"values\": [1e-4, 1e-3, 1e-2]\n",
    "        #},\n",
    "        \"a2\": {\n",
    "            \"values\": [0.0, 1e-3, 1e-2, 1e-1, 1e0]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd2d039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sweep\n",
    "sweep_id = wandb.sweep(hp_study, project=wandb_project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f962e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data from wandb\n",
    "file_ext = \".pkl\"\n",
    "\n",
    "api = wandb.Api()\n",
    "\n",
    "runs = api.runs(wandb_project)\n",
    "for run in runs:\n",
    "    for f in run.files():\n",
    "        if f.name[-len(file_ext):] == file_ext and not os.path.isfile(f.name):\n",
    "            print(f\"Downloading {os.path.basename(f.name)}.\")\n",
    "            run.file(f.name).download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2baaa94a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c0e5f43192f4d4abd8dad9bb315e8e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot losses\n",
    "save_names = [\"srnet_model_F00_bn.pkl\", \"F00_bn_norm_a2\"]\n",
    "save_path = \"models\"\n",
    "models = ut.plot_losses(save_names, save_path=\"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "06abffac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data_path = \"data_1k\"\n",
    "\n",
    "in_var = \"X00\"\n",
    "lat_var = \"G00\"\n",
    "target_var = \"F00\"\n",
    "\n",
    "mask_ext = \".mask\"\n",
    "masks = joblib.load(os.path.join(data_path, in_var + mask_ext))     # TODO: create mask if file does not exist\n",
    "\n",
    "train_data = SRData(data_path, in_var, lat_var, target_var, masks[\"train\"])\n",
    "val_data = SRData(data_path, in_var, lat_var, target_var, masks[\"val\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9ecafdc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "srnet_model_F00_bn\n",
      "[5.482872, 0.33377925, 1.2634401e-06]\n",
      "[1, 0, 2]\n",
      "[[-2.1264930e+00  9.6783280e-01]\n",
      " [ 1.5003376e+00 -6.6390908e-01]\n",
      " [-3.6289787e-01 -7.9980089e-10]]\n",
      "[[-2.1264930e+00  9.6783280e-01]\n",
      " [ 1.5003376e+00 -6.6390908e-01]\n",
      " [-3.6289787e-01 -7.9980089e-10]]\n",
      "\n",
      "srnet_model_F00_bn_norm_a2_0e+00\n",
      "[0.7364508, 0.48084727, 0.23385297]\n",
      "[1, 0, 2]\n",
      "[[-1.5839932   0.51744026]\n",
      " [ 0.97438246 -0.5205308 ]\n",
      " [-0.8817944  -0.97543824]]\n",
      "[[0.7439408  0.25605917]\n",
      " [0.6115546  0.3884454 ]\n",
      " [0.47660613 0.52339387]]\n",
      "\n",
      "srnet_model_F00_bn_norm_a2_1e+00\n",
      "[0.78763723, 0.40730256, 0.172619]\n",
      "[1, 0, 2]\n",
      "[[ 1.32152272e-05 -1.13696697e-05]\n",
      " [ 3.54378926e-06 -2.50317498e-06]\n",
      " [ 1.25902352e-06  1.42729195e-05]]\n",
      "[[0.5000005  0.49999955]\n",
      " [0.50000024 0.49999973]\n",
      " [0.49999678 0.5000033 ]]\n",
      "\n",
      "srnet_model_F00_bn_norm_a2_1e-01\n",
      "[0.6218816, 0.37553275, 0.2425685]\n",
      "[1, 0, 2]\n",
      "[[-3.2783086e-05 -1.7600223e-05]\n",
      " [ 6.1090641e-06  8.7805711e-06]\n",
      " [-9.0448593e-06  2.8484001e-06]]\n",
      "[[0.5000038  0.49999622]\n",
      " [0.4999993  0.50000066]\n",
      " [0.50000155 0.49999845]]\n",
      "\n",
      "srnet_model_F00_bn_norm_a2_1e-02\n",
      "[0.6494187, 0.46756876, 0.13967201]\n",
      "[1, 0, 2]\n",
      "[[ 7.4458731e-05  3.5313198e-05]\n",
      " [-4.5851597e-05 -8.5120671e-05]\n",
      " [-5.2859677e-06 -5.7988600e-06]]\n",
      "[[0.5000098  0.4999902 ]\n",
      " [0.4999902  0.50000983]\n",
      " [0.49999985 0.5000001 ]]\n",
      "\n",
      "srnet_model_F00_bn_norm_a2_1e-03\n",
      "[0.5693084, 0.47717938, 0.19789289]\n",
      "[1, 0, 2]\n",
      "[[-2.4333000e-01 -6.8523492e-05]\n",
      " [ 2.1791556e-01  2.7145990e-04]\n",
      " [ 2.1127063e-04 -1.8615976e-02]]\n",
      "[[0.56051725 0.4394828 ]\n",
      " [0.55419725 0.44580272]\n",
      " [0.49539897 0.50460106]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# overview latent feature variance and alpha matrix\n",
    "model_path = \"models\"\n",
    "model_ext = \".pkl\"\n",
    "\n",
    "alpha_eps = 1e-6\n",
    "alpha_bin = False\n",
    "\n",
    "for model_name in models:\n",
    "    print(model_name)\n",
    "    model = ut.load_model(model_name + model_ext, model_path, SRNet)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        preds, acts = model(train_data.in_data, get_lat=True)\n",
    "        \n",
    "    all_nodes = ut.get_node_order(acts, show=True)\n",
    "    \n",
    "    if alpha_eps:\n",
    "        alpha = model.layers1.alpha.detach().cpu().numpy()[all_nodes]\n",
    "        \n",
    "        if alpha_bin:\n",
    "            alpha[np.abs(alpha) < alpha_eps] = 0\n",
    "            alpha[np.abs(alpha) > alpha_eps] = 1\n",
    "        \n",
    "        print(alpha[np.abs(alpha).sum(axis=1) > alpha_eps])\n",
    "    \n",
    "    print(model.layers1.norm(model.layers1.alpha.detach().cpu()).numpy()[all_nodes])\n",
    "    \n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09f65c3",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "* Just `softmax` normalization, i.e. $a_2 = 0$, does not yield any improvements over the standard bottleneck DSN\n",
    "\n",
    "* Actually, regularization of $\\alpha$ via $a_2$ does not make any sense when `softmax` normalization is applied afterwards\n",
    "\n",
    "**TODO**:\n",
    "\n",
    "* Apply regularization via $a_2$ after `softmax`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e34f82",
   "metadata": {},
   "source": [
    "### Step 7.4: Train bottleneck DSN with $\\hat{\\alpha}$ and $e_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "329e720f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set wandb project\n",
    "wandb_project = \"74-bn-DSN-norm-e1-study-F00\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9299fc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams = {\n",
    "#     \"arch\": {\n",
    "#         \"in_size\": train_data.in_data.shape[1],\n",
    "#         \"out_size\": train_data.target_data.shape[1],\n",
    "#         \"hid_num\": (2,0),\n",
    "#         \"hid_size\": 32, \n",
    "#         \"hid_type\": (\"DSN\", \"MLP\"),\n",
    "#         \"hid_kwargs\": ({\"norm\": \"softmax\"}, {}),\n",
    "#         \"lat_size\": 3,\n",
    "#         },\n",
    "#     \"epochs\": 10000,\n",
    "#     \"runtime\": None,\n",
    "#     \"batch_size\": 64,\n",
    "#     \"lr\": 1e-4,\n",
    "#     \"wd\": 1e-4,\n",
    "#     \"l1\": 0.0,\n",
    "#     \"a1\": 0.0,\n",
    "#     \"a2\": 0.0,\n",
    "#     \"e1\": 0.0,\n",
    "#     \"gc\": 0.0,\n",
    "#     \"shuffle\": True,\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b6fe8c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameter study\n",
    "hp_study = {\n",
    "    \"method\": \"grid\", # random, bayesian\n",
    "    #\"metric\": {\n",
    "    #    \"name\": \"val_loss\",\n",
    "    #    \"goal\": \"minimize\",\n",
    "    #},\n",
    "    \"parameters\": {\n",
    "        #\"l1\": {\n",
    "        #    \"values\": [1e-4, 1e-3, 1e-2]\n",
    "        #},\n",
    "        \"e1\": {\n",
    "            \"values\": [0.0, 1e-5, 1e-3, 1e-2, 1e-1, 1e1]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a385bd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sweep\n",
    "sweep_id = wandb.sweep(hp_study, project=wandb_project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c44e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data from wandb\n",
    "file_ext = \".pkl\"\n",
    "\n",
    "api = wandb.Api()\n",
    "\n",
    "runs = api.runs(wandb_project)\n",
    "for run in runs:\n",
    "    for f in run.files():\n",
    "        if f.name[-len(file_ext):] == file_ext and not os.path.isfile(f.name):\n",
    "            print(f\"Downloading {os.path.basename(f.name)}.\")\n",
    "            run.file(f.name).download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cc4c3362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "032101ef09344223854c4a2d79728be3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot losses\n",
    "save_names = [\"F00_bn_norm_e1\"]\n",
    "save_path = \"models\"\n",
    "models = ut.plot_losses(save_names, save_path=\"models\", excl_names=[\"max\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "af27a64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data_path = \"data_1k\"\n",
    "\n",
    "in_var = \"X00\"\n",
    "lat_var = \"G00\"\n",
    "target_var = \"F00\"\n",
    "\n",
    "mask_ext = \".mask\"\n",
    "masks = joblib.load(os.path.join(data_path, in_var + mask_ext))     # TODO: create mask if file does not exist\n",
    "\n",
    "train_data = SRData(data_path, in_var, lat_var, target_var, masks[\"train\"])\n",
    "val_data = SRData(data_path, in_var, lat_var, target_var, masks[\"val\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "12d62c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "srnet_model_F00_bn_norm_e1_0e+00\n",
      "[0.7364508, 0.48084727, 0.23385297]\n",
      "[1, 0, 2]\n",
      "[[0.7439408  0.25605917]\n",
      " [0.6115546  0.3884454 ]\n",
      " [0.47660613 0.52339387]]\n",
      "\n",
      "srnet_model_F00_bn_norm_e1_1e+01\n",
      "[2.45539, 0.38972935, 0.14927715]\n",
      "[1, 0, 2]\n",
      "[[9.9946922e-01 5.3078489e-04]\n",
      " [9.9723595e-01 2.7640408e-03]\n",
      " [1.6764498e-03 9.9832350e-01]]\n",
      "\n",
      "srnet_model_F00_bn_norm_e1_1e-01\n",
      "[0.7481753, 0.7479166, 0.62162703]\n",
      "[2, 1, 0]\n",
      "[[0.0083454  0.99165463]\n",
      " [0.9936801  0.00631982]\n",
      " [0.97826666 0.02173341]]\n",
      "\n",
      "srnet_model_F00_bn_norm_e1_1e-02\n",
      "[1.1345829, 0.62815154, 0.2887137]\n",
      "[1, 0, 2]\n",
      "[[0.97732645 0.02267359]\n",
      " [0.93166333 0.06833664]\n",
      " [0.0288407  0.9711593 ]]\n",
      "\n",
      "srnet_model_F00_bn_norm_e1_1e-03\n",
      "[0.66432446, 0.51293874, 0.23554066]\n",
      "[1, 0, 2]\n",
      "[[0.9051772  0.09482283]\n",
      " [0.75741494 0.2425851 ]\n",
      " [0.3511344  0.6488656 ]]\n",
      "\n",
      "srnet_model_F00_bn_norm_e1_1e-05\n",
      "[0.7657546, 0.46918085, 0.23475884]\n",
      "[1, 0, 2]\n",
      "[[0.74743396 0.25256613]\n",
      " [0.61274755 0.38725242]\n",
      " [0.47625598 0.52374405]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# overview latent feature variance and alpha matrix\n",
    "model_path = \"models\"\n",
    "model_ext = \".pkl\"\n",
    "\n",
    "alpha_eps = None\n",
    "alpha_bin = False\n",
    "\n",
    "for model_name in models:\n",
    "    print(model_name)\n",
    "    model = ut.load_model(model_name + model_ext, model_path, SRNet)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        preds, acts = model(train_data.in_data, get_lat=True)\n",
    "        \n",
    "    all_nodes = ut.get_node_order(acts, show=True)\n",
    "    \n",
    "    if alpha_eps:\n",
    "        alpha = model.layers1.alpha.detach().cpu().numpy()[all_nodes]\n",
    "        \n",
    "        if alpha_bin:\n",
    "            alpha[np.abs(alpha) < alpha_eps] = 0\n",
    "            alpha[np.abs(alpha) > alpha_eps] = 1\n",
    "        \n",
    "        print(alpha[np.abs(alpha).sum(axis=1) > alpha_eps])\n",
    "        \n",
    "    print(model.layers1.norm(model.layers1.alpha.detach().cpu()).numpy()[all_nodes])\n",
    "    \n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce52b39",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "* Minimizing entropy seems to be efficient\n",
    "\n",
    "* For $e_2$ being `1e-01` or larger, the validation error is poor\n",
    "\n",
    "* For `1e-05`, sparse dependencies on the input features are not enforced\n",
    "\n",
    "* Even `1e-03` appears to be too low, as two latent features still depend on two input features\n",
    "\n",
    "* For `1e-02`, the lowest validation error is achieved and all latent features depend largely on a single input feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8c139a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.1345829, 0.62815154, 0.2887137]\n",
      "[1, 0, 2]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"srnet_model_F00_bn_norm_e1_1e-02\"\n",
    "\n",
    "model = ut.load_model(model_name + model_ext, model_path, SRNet)\n",
    "\n",
    "with torch.no_grad():\n",
    "    preds, acts = model(train_data.in_data, get_lat=True)\n",
    "    \n",
    "all_nodes = ut.get_node_order(acts, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2ddae627",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = all_nodes[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cc1808a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select plotting data\n",
    "x_data = train_data.in_data[:,0]\n",
    "y_data = train_data.in_data[:,1]\n",
    "z_data = [\n",
    "    (\"target\", train_data.target_data),\n",
    "    #(\"x**2\", x_data**2), \n",
    "    #(\"cos(y)\", np.cos(y_data)), \n",
    "    #(\"x*y\", x_data * y_data),\n",
    "]\n",
    "plot_size = train_data.target_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5f9952ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fc61ee3aa1341acbb1dd85239ca5e71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ut.plot_acts(x_data, y_data, z_data, acts=acts, nodes=nodes, model=model, bias=True, nonzero=False, agg=False, plot_size=plot_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6604a041",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_data = [\n",
    "    (\"x**2\", x_data**2), \n",
    "    (\"cos(y)\", np.cos(y_data)), \n",
    "    (\"x*y\", x_data * y_data),\n",
    "    (\"x\", x_data),\n",
    "    (\"y\", y_data),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d2c47f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Node 1\n",
      "corr(n1, x**2): -0.6755\n",
      "corr(n1, cos(y)): 0.0114\n",
      "corr(n1, x*y): -0.1712\n",
      "corr(n1, x): -0.8817\n",
      "corr(n1, y): -0.0393\n",
      "\n",
      "Node 0\n",
      "corr(n0, x**2): -0.6556\n",
      "corr(n0, cos(y)): -0.0993\n",
      "corr(n0, x*y): -0.6574\n",
      "corr(n0, x): 0.1075\n",
      "corr(n0, y): 0.3171\n",
      "\n",
      "Node 2\n",
      "corr(n2, x**2): 0.0314\n",
      "corr(n2, cos(y)): 0.3564\n",
      "corr(n2, x*y): 0.0860\n",
      "corr(n2, x): -0.0281\n",
      "corr(n2, y): 0.8895\n"
     ]
    }
   ],
   "source": [
    "ut.node_correlations(acts, nodes, corr_data);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6e2216",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "* The model resulting from $e_2$ being `1e-02` has clearer dependencies:\n",
    "\n",
    "    * Node 1: $\\approx x^2$\n",
    "    \n",
    "    * Node 0: $\\approx x \\cdot y$\n",
    "    \n",
    "    * Node 2: $\\approx \\text{cos}(y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d6928293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3775b5a793cf4d70b88b7649d1c5ffc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "n = 1\n",
    "bias = True\n",
    "\n",
    "ax.scatter(x_data, x_data**2)\n",
    "# ax.scatter(x_data, acts[:,n])\n",
    "ax.scatter(x_data, model.layers2[0].weight[0,n].item()*acts[:,n] + bias * model.layers2[0].bias.item())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f8185f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select plotting data\n",
    "x_data = train_data.in_data[:,0]\n",
    "y_data = train_data.in_data[:,1]\n",
    "z_data = [\n",
    "    #(\"target\", train_data.target_data),\n",
    "    #(\"x**2\", x_data**2), \n",
    "    #(\"cos(y)\", np.cos(y_data)), \n",
    "    (\"x*y\", x_data * y_data),\n",
    "]\n",
    "plot_size = train_data.target_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "23b4e5a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dae9a76f08d44ec391319bb5ece514ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ut.plot_acts(x_data, y_data, z_data, acts=acts, nodes=[0], model=model, bias=True, nonzero=False, agg=False, plot_size=plot_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9fe8f007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "972ea6fc0bc14e7f870194282ed052c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "n = 2\n",
    "bias = True\n",
    "\n",
    "ax.scatter(y_data, np.cos(y_data))\n",
    "# ax.scatter(y_data, acts[:,n])\n",
    "ax.scatter(y_data, model.layers2[0].weight[0,n].item()*acts[:,n] + bias * model.layers2[0].bias.item())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8eb1a1",
   "metadata": {},
   "source": [
    "Although we learn clearer dependencies, we can observe the impact of degeneracies. \n",
    "\n",
    "For example, node 0 primarily models $x \\cdot y$, but also contains a part of $x^2$. At this point, the symbolic discriminator should come in to incentivize the desired shapes.\n",
    "\n",
    "A few questions remain:\n",
    "\n",
    "1. How is $x \\cdot y$ realized when all latent features largely depend on a single input feature? Is there a weight that amplifies the signal masked by $\\alpha$?\n",
    "\n",
    "2. Can we prune/limit the $\\alpha$ mask to get strict dependencies on single input features?\n",
    "\n",
    "3. What happens when we train more epochs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dece8346",
   "metadata": {},
   "source": [
    "**Question 1**: How is $x \\cdot y$ realized when all latent features largely depend on a single input feature? Is there a weight that amplifies the signal masked by $\\alpha$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "916f1895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9317, 0.0683],\n",
       "        [0.9773, 0.0227],\n",
       "        [0.0288, 0.9712]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers1.norm(model.layers1.alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1b734c3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([0.7276, 1.1862], grad_fn=<MaxBackward0>),\n",
       "indices=tensor([27, 17]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers1.w[0][0,...].abs().max(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a5ed0d74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([0.8105, 0.0571], grad_fn=<MaxBackward0>),\n",
       "indices=tensor([16, 16]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers1.w[0][1,...].abs().max(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "74fde826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([0.7636, 0.6659], grad_fn=<MaxBackward0>),\n",
       "indices=tensor([16,  0]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers1.w[0][2,...].abs().max(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72d01fc",
   "metadata": {},
   "source": [
    "Node 0 shows a slight amplification of $y$.\n",
    "\n",
    "**Question 2**: Can we prune/limit the $\\alpha$ mask to get strict dependencies on single input features?\n",
    "\n",
    "Implement as threshold? https://pytorch.org/docs/stable/generated/torch.nn.Threshold.html#torch.nn.Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c0a41893",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_org = model\n",
    "acts_org = acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b2f32be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9102438, 0.6963518, 0.27245826]\n",
      "[1, 0, 2]\n",
      "[[0.9773646  0.0226354 ]\n",
      " [0.93092346 0.06907651]\n",
      " [0.02262298 0.977377  ]]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"srnet_model_F00_bn_norm_prune_5e-02_e2_1e-02\"\n",
    "\n",
    "model = ut.load_model(model_name + model_ext, model_path, SRNet)\n",
    "\n",
    "with torch.no_grad():\n",
    "    preds, acts = model(train_data.in_data, get_lat=True)\n",
    "    \n",
    "all_nodes = ut.get_node_order(acts, show=True)\n",
    "\n",
    "print(model.layers1.norm(model.layers1.alpha.detach().cpu()).numpy()[all_nodes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6f5fd22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = all_nodes[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7a916a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_data = [\n",
    "    (\"x**2\", x_data**2), \n",
    "    (\"cos(y)\", np.cos(y_data)), \n",
    "    (\"x*y\", x_data * y_data),\n",
    "    (\"x\", x_data),\n",
    "    (\"y\", y_data),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a95af732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Node 1\n",
      "corr(n1, x**2): -0.6816\n",
      "corr(n1, cos(y)): 0.0121\n",
      "corr(n1, x*y): -0.1724\n",
      "corr(n1, x): -0.8793\n",
      "corr(n1, y): -0.0422\n",
      "\n",
      "Node 0\n",
      "corr(n0, x**2): -0.6845\n",
      "corr(n0, cos(y)): -0.1080\n",
      "corr(n0, x*y): -0.6801\n",
      "corr(n0, x): 0.0711\n",
      "corr(n0, y): 0.2504\n",
      "\n",
      "Node 2\n",
      "corr(n2, x**2): 0.0318\n",
      "corr(n2, cos(y)): 0.3954\n",
      "corr(n2, x*y): 0.0255\n",
      "corr(n2, x): 0.0256\n",
      "corr(n2, y): 0.8748\n"
     ]
    }
   ],
   "source": [
    "ut.node_correlations(acts, nodes, corr_data);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b6aea82b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2558d26e373d44c7853a416ef4695e5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "n = 1\n",
    "bias = True\n",
    "\n",
    "ax.scatter(x_data, x_data**2, label=\"Target: $x_0^2$\")\n",
    "ax.scatter(x_data, model_org.layers2[0].weight[0,n].item()*acts_org[:,n] + bias * model_org.layers2[0].bias.item(), label=\"DSN Entropy\")\n",
    "ax.scatter(x_data, model.layers2[0].weight[0,n].item()*acts[:,n] + bias * model.layers2[0].bias.item(), label=\"DSN Entropy + Pruning\")\n",
    "\n",
    "ax.set_xlabel(\"$x_0$\")\n",
    "ax.set_ylabel(\"$g_0$\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "34ead2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select plotting data\n",
    "x_data = train_data.in_data[:,0]\n",
    "y_data = train_data.in_data[:,1]\n",
    "z_data = [\n",
    "    #(\"target\", train_data.target_data),\n",
    "    #(\"x**2\", x_data**2), \n",
    "    #(\"cos(y)\", np.cos(y_data)), \n",
    "    (\"x*y\", x_data * y_data),\n",
    "]\n",
    "plot_size = train_data.target_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2ddafeda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98024130ce8a4116866dccbebbd94b6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "x_data = train_data.in_data[:,0][:plot_size].numpy()\n",
    "y_data = train_data.in_data[:,1][:plot_size].numpy()\n",
    "z_data = [(\"x*y\", x_data * y_data)]\n",
    "\n",
    "for d in z_data:\n",
    "    z_data = d[1]\n",
    "    ax.scatter3D(x_data, y_data, z_data, label=\"Target: $x_0 \\cdot x_1$\")\n",
    "\n",
    "n = 0\n",
    "b = model_org.layers2[0]._parameters['bias'].item()\n",
    "w = model_org.layers2[0]._parameters['weight'].detach().numpy()[0, n]\n",
    "n_data = w * acts_org[:,n][:plot_size].numpy() + b\n",
    "ax.scatter3D(x_data, y_data, n_data, label=\"DSN Entropy\")\n",
    "    \n",
    "n = 0\n",
    "b = model.layers2[0]._parameters['bias'].item()\n",
    "w = model.layers2[0]._parameters['weight'].detach().numpy()[0, n]\n",
    "n_data = w * acts[:,n][:plot_size].numpy() + b\n",
    "ax.scatter3D(x_data, y_data, n_data, label=\"DSN Entropy + Pruning\")\n",
    "\n",
    "ax.set_xlabel(\"$x_0$\")\n",
    "ax.set_ylabel(\"$x_1$\")\n",
    "ax.set_zlabel(\"$g_2$\")\n",
    "ax.view_init(elev=5.16234, azim=-56.591)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "98ac0464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "752a2ddbefef4a408affe41cf5a3cb35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "n = 2\n",
    "bias = True\n",
    "\n",
    "ax.scatter(y_data, np.cos(y_data), label=\"Target: cos$(x_1)$\")\n",
    "ax.scatter(y_data, model_org.layers2[0].weight[0,n].item()*acts_org[:,n] + bias * model_org.layers2[0].bias.item(), label=\"DSN Entropy\")\n",
    "ax.scatter(y_data, model.layers2[0].weight[0,n].item()*acts[:,n] + bias * model.layers2[0].bias.item(), label=\"DSN Entropy + Pruning\")\n",
    "\n",
    "ax.set_xlabel(\"$x_1$\")\n",
    "ax.set_ylabel(\"$g_1$\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263d1fe9",
   "metadata": {},
   "source": [
    "Pruning yields very similar latent features with slightly clearer dependencies on the input features.\n",
    "\n",
    "**Question 3**: What happens when we train more epochs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c104a57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data from wandb\n",
    "file_ext = \".pkl\"\n",
    "\n",
    "api = wandb.Api()\n",
    "\n",
    "runs = api.runs(wandb_project)\n",
    "for run in runs:\n",
    "    for f in run.files():\n",
    "        if f.name[-len(file_ext):] == file_ext and not os.path.isfile(f.name):\n",
    "            print(f\"Downloading {os.path.basename(f.name)}.\")\n",
    "            run.file(f.name).download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d6a0082b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeb6f17224dd464fa0caca95658e5585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot losses\n",
    "save_names = [\"F00_bn_norm_e1_1e-02_max\"]\n",
    "save_path = \"models\"\n",
    "models = ut.plot_losses(save_names, save_path=\"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "94bf8d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data_path = \"data_1k\"\n",
    "\n",
    "in_var = \"X00\"\n",
    "lat_var = \"G00\"\n",
    "target_var = \"F00\"\n",
    "\n",
    "mask_ext = \".mask\"\n",
    "masks = joblib.load(os.path.join(data_path, in_var + mask_ext))     # TODO: create mask if file does not exist\n",
    "\n",
    "train_data = SRData(data_path, in_var, lat_var, target_var, masks[\"train\"])\n",
    "val_data = SRData(data_path, in_var, lat_var, target_var, masks[\"val\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7d75cd42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "srnet_model_F00_bn_norm_e1_1e-02_max\n",
      "[0.28535384, 0.071490675, 0.0]\n",
      "[0, 2, 1]\n",
      "[[0.9474744  0.05252557]\n",
      " [0.02267693 0.97732306]\n",
      " [0.97731006 0.02268998]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# overview latent feature variance and alpha matrix\n",
    "model_path = \"models\"\n",
    "model_ext = \".pkl\"\n",
    "\n",
    "alpha_eps = None\n",
    "alpha_bin = False\n",
    "\n",
    "for model_name in models:\n",
    "    print(model_name)\n",
    "    model = ut.load_model(model_name + model_ext, model_path, SRNet)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        preds, acts = model(train_data.in_data, get_lat=True)\n",
    "        \n",
    "    all_nodes = ut.get_node_order(acts, show=True)\n",
    "    \n",
    "    if alpha_eps:\n",
    "        alpha = model.layers1.alpha.detach().cpu().numpy()[all_nodes]\n",
    "        \n",
    "        if alpha_bin:\n",
    "            alpha[np.abs(alpha) < alpha_eps] = 0\n",
    "            alpha[np.abs(alpha) > alpha_eps] = 1\n",
    "        \n",
    "        print(alpha[np.abs(alpha).sum(axis=1) > alpha_eps])\n",
    "    \n",
    "    print(model.layers1.norm(model.layers1.alpha.detach().cpu()).numpy()[all_nodes])\n",
    "    \n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d705564a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.28535384, 0.071490675, 0.0]\n",
      "[0, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"srnet_model_F00_bn_norm_e1_1e-02_max\"\n",
    "\n",
    "model = ut.load_model(model_name + model_ext, model_path, SRNet)\n",
    "\n",
    "with torch.no_grad():\n",
    "    preds, acts = model(train_data.in_data, get_lat=True)\n",
    "    \n",
    "all_nodes = ut.get_node_order(acts, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f5118c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = all_nodes[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "167f8506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select plotting data\n",
    "x_data = train_data.in_data[:,0]\n",
    "y_data = train_data.in_data[:,1]\n",
    "z_data = [\n",
    "    (\"target\", train_data.target_data),\n",
    "    #(\"x**2\", x_data**2), \n",
    "    #(\"cos(y)\", np.cos(y_data)), \n",
    "    #(\"x*y\", x_data * y_data),\n",
    "]\n",
    "plot_size = train_data.target_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6594aede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9df032af9cd44af820550e2ac463537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ut.plot_acts(x_data, y_data, z_data, acts=acts, nodes=nodes, model=model, bias=True, nonzero=False, agg=False, plot_size=plot_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8e4e2d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_data = [\n",
    "    (\"x**2\", x_data**2), \n",
    "    (\"cos(y)\", np.cos(y_data)), \n",
    "    (\"x*y\", x_data * y_data),\n",
    "    (\"x\", x_data),\n",
    "    (\"y\", y_data),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ed5a353f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Node 0\n",
      "corr(n0, x**2): -0.8425\n",
      "corr(n0, cos(y)): -0.0667\n",
      "corr(n0, x*y): -0.6543\n",
      "corr(n0, x): -0.2661\n",
      "corr(n0, y): 0.0567\n",
      "\n",
      "Node 2\n",
      "corr(n2, x**2): 0.0138\n",
      "corr(n2, cos(y)): 0.7685\n",
      "corr(n2, x*y): -0.0103\n",
      "corr(n2, x): 0.0122\n",
      "corr(n2, y): 0.5615\n"
     ]
    }
   ],
   "source": [
    "ut.node_correlations(acts, nodes, corr_data);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "603d1b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select plotting data\n",
    "x_data = train_data.in_data[:,0]\n",
    "y_data = train_data.in_data[:,1]\n",
    "z_data = [\n",
    "    #(\"target\", train_data.target_data),\n",
    "    #(\"x**2\", x_data**2), \n",
    "    #(\"cos(y)\", np.cos(y_data)), \n",
    "    (\"x*y\", x_data * y_data),\n",
    "]\n",
    "plot_size = train_data.target_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6bedcbf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de6060c1f2de4c0ab6ab205dee03408a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ut.plot_acts(x_data, y_data, z_data, acts=acts, nodes=[0], model=model, bias=True, nonzero=False, agg=False, plot_size=plot_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f961431d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f53328bc0a45491ca02e6bbdc6bfb023",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(y_data, np.cos(y_data))\n",
    "ax.scatter(y_data, acts[:,2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665cb5cd",
   "metadata": {},
   "source": [
    "When training for 100k epochs with $e_1$ being `1e-2`, the latent features seem to converge to $x^2 + x \\cdot y$ and $\\text{cos}(y)$. Why does one latent feature have zero variance? Weight decay? Its $\\alpha$ values are not zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b6b745",
   "metadata": {},
   "source": [
    "**Next steps**:\n",
    "\n",
    "* Symbolic discriminator\n",
    "\n",
    "* Resolve bottleneck\n",
    "\n",
    "* Ensure long epoch convergence\n",
    "\n",
    "* Increase input feature dimension\n",
    "\n",
    "* Change $f(x)$\n",
    "\n",
    "* Check AI Feynman: https://github.com/SJ001/AI-Feynman\n",
    "\n",
    "* Techniques:\n",
    "\n",
    "    * Consider two step training\n",
    "\n",
    "    * Regularize mean instead of summed entropy (of high-variance features)\n",
    "    \n",
    "    * Entropy $\\times$ variance\n",
    "    \n",
    "    * Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637a1396",
   "metadata": {},
   "source": [
    "**Work-in-progress**:\n",
    "\n",
    "### Step 7.5: Train bottleneck DSN with $\\hat{\\alpha}$ and $e_2$\n",
    "\n",
    "We want to penalize `F.softmax(lat_acts.var(dim=0)) * entropy`, not `lat_acts.var(dim=0) * entropy`.\n",
    "\n",
    "However, this might be more effective for non-bottleneck DSNs.\n",
    "\n",
    "Takeaway: less clear dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8345ef6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set wandb project\n",
    "wandb_project = \"75-bn-DSN-norm-e2-study-F00\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b898c2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams = {\n",
    "#     \"arch\": {\n",
    "#         \"in_size\": train_data.in_data.shape[1],\n",
    "#         \"out_size\": train_data.target_data.shape[1],\n",
    "#         \"hid_num\": (2,0),\n",
    "#         \"hid_size\": 32, \n",
    "#         \"hid_type\": (\"DSN\", \"MLP\"),\n",
    "#         \"hid_kwargs\": ({\"norm\": \"softmax\"}, {}),\n",
    "#         \"lat_size\": 3,\n",
    "#         },\n",
    "#     \"epochs\": 10000,\n",
    "#     \"runtime\": None,\n",
    "#     \"batch_size\": 64,\n",
    "#     \"lr\": 1e-4,\n",
    "#     \"wd\": 1e-4,\n",
    "#     \"l1\": 0.0,\n",
    "#     \"a1\": 0.0,\n",
    "#     \"a2\": 0.0,\n",
    "#     \"e1\": 0.0,\n",
    "#     \"e2\": 0.0,\n",
    "#     \"gc\": 0.0,\n",
    "#     \"shuffle\": True,\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62342aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameter study\n",
    "hp_study = {\n",
    "    \"method\": \"grid\", # random, bayesian\n",
    "    #\"metric\": {\n",
    "    #    \"name\": \"val_loss\",\n",
    "    #    \"goal\": \"minimize\",\n",
    "    #},\n",
    "    \"parameters\": {\n",
    "        #\"l1\": {\n",
    "        #    \"values\": [1e-4, 1e-3, 1e-2]\n",
    "        #},\n",
    "        \"e2\": {\n",
    "            \"values\": [0.0, 1e-5, 1e-3, 1e-1, 1e1]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1b6d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sweep\n",
    "sweep_id = wandb.sweep(hp_study, project=wandb_project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b010cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data from wandb\n",
    "file_ext = \".pkl\"\n",
    "\n",
    "api = wandb.Api()\n",
    "\n",
    "runs = api.runs(wandb_project)\n",
    "for run in runs:\n",
    "    for f in run.files():\n",
    "        if f.name[-len(file_ext):] == file_ext and not os.path.isfile(f.name):\n",
    "            print(f\"Downloading {os.path.basename(f.name)}.\")\n",
    "            run.file(f.name).download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73be69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot losses\n",
    "save_names = [\"F00_bn_norm_e2\"]\n",
    "save_path = \"models\"\n",
    "models = ut.plot_losses(save_names, save_path=\"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187078f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data_path = \"data_1k\"\n",
    "\n",
    "in_var = \"X00\"\n",
    "lat_var = \"G00\"\n",
    "target_var = \"F00\"\n",
    "\n",
    "mask_ext = \".mask\"\n",
    "masks = joblib.load(os.path.join(data_path, in_var + mask_ext))     # TODO: create mask if file does not exist\n",
    "\n",
    "train_data = SRData(data_path, in_var, lat_var, target_var, masks[\"train\"])\n",
    "val_data = SRData(data_path, in_var, lat_var, target_var, masks[\"val\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c216bd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overview latent feature variance and alpha matrix\n",
    "model_path = \"models\"\n",
    "model_ext = \".pkl\"\n",
    "\n",
    "alpha_eps = 1e-2\n",
    "alpha_bin = False\n",
    "\n",
    "for model_name in models:\n",
    "    print(model_name)\n",
    "    model = ut.load_model(model_name + model_ext, model_path, SRNet)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        preds, acts = model(train_data.in_data, get_lat=True)\n",
    "        \n",
    "    all_nodes = ut.get_node_order(acts, show=True)\n",
    "    \n",
    "    if alpha_eps:\n",
    "        alpha = model.layers1.alpha.detach().cpu().numpy()[all_nodes]\n",
    "        \n",
    "        if alpha_bin:\n",
    "            alpha[np.abs(alpha) < alpha_eps] = 0\n",
    "            alpha[np.abs(alpha) > alpha_eps] = 1\n",
    "        \n",
    "        print(alpha[np.abs(alpha).sum(axis=1) > alpha_eps])\n",
    "        \n",
    "    print(model.layers1.alpha_n.detach().cpu().numpy()[all_nodes])\n",
    "    \n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7c543e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"srnet_model_F00_bn_norm_e2_1e+01\"\n",
    "\n",
    "model = ut.load_model(model_name + model_ext, model_path, SRNet)\n",
    "\n",
    "with torch.no_grad():\n",
    "    preds, acts = model(train_data.in_data, get_lat=True)\n",
    "    \n",
    "all_nodes = ut.get_node_order(acts, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f698fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = all_nodes[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02762f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select plotting data\n",
    "x_data = train_data.in_data[:,0]\n",
    "y_data = train_data.in_data[:,1]\n",
    "z_data = [\n",
    "    (\"target\", train_data.target_data),\n",
    "    #(\"x**2\", x_data**2), \n",
    "    #(\"cos(y)\", np.cos(y_data)), \n",
    "    #(\"x*y\", x_data * y_data),\n",
    "]\n",
    "plot_size = train_data.target_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535d1c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "ut.plot_acts(x_data, y_data, z_data, acts=acts, nodes=nodes, model=model, bias=True, nonzero=False, agg=False, plot_size=plot_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30d446d",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_data = [\n",
    "    (\"x**2\", x_data**2), \n",
    "    (\"cos(y)\", np.cos(y_data)), \n",
    "    (\"x*y\", x_data * y_data),\n",
    "    (\"x\", x_data),\n",
    "    (\"y\", y_data),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2d2ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ut.node_correlations(acts, nodes, corr_data, nonzero=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1f54f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(x_data, x_data**2)\n",
    "ax.scatter(x_data, acts[:,1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb5e537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select plotting data\n",
    "x_data = train_data.in_data[:,0]\n",
    "y_data = train_data.in_data[:,1]\n",
    "z_data = [\n",
    "    #(\"target\", train_data.target_data),\n",
    "    #(\"x**2\", x_data**2), \n",
    "    #(\"cos(y)\", np.cos(y_data)), \n",
    "    (\"x*y\", x_data * y_data),\n",
    "]\n",
    "plot_size = train_data.target_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0531d4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ut.plot_acts(x_data, y_data, z_data, acts=acts, nodes=[0], model=model, bias=True, nonzero=False, agg=False, plot_size=plot_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85c6d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(y_data, np.cos(y_data))\n",
    "ax.scatter(y_data, acts[:,0])\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
